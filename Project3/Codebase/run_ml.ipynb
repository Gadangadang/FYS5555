{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fewer-prior",
   "metadata": {},
   "source": [
    "# ML test\n",
    "### Testing of reading in data and trying an auto encoder\n",
    "Remember to pip3 install keras-tuner to tune for the given session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "homeless-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "seed = tf.random.set_seed(1)\n",
    "#import ROOT as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fallen-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"/storage/shared/data/2lep_df_forML.hdf5\")\n",
    "df = pd.concat([df,pd.read_hdf(\"/storage/shared/data/2lep_df_forML_signal.hdf5\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cross-greenhouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 109683372 entries, 0 to 2785053\n",
      "Data columns (total 21 columns):\n",
      " #   Column    Dtype  \n",
      "---  ------    -----  \n",
      " 0   lep_pt1   float64\n",
      " 1   lep_eta1  float64\n",
      " 2   lep_phi1  float64\n",
      " 3   lep_E1    float64\n",
      " 4   lep_pt2   float64\n",
      " 5   lep_eta2  float64\n",
      " 6   lep_phi2  float64\n",
      " 7   lep_E2    float64\n",
      " 8   met       float64\n",
      " 9   mll       float64\n",
      " 10  njet20    int64  \n",
      " 11  njet60    int64  \n",
      " 12  nbjet60   int64  \n",
      " 13  nbjet70   int64  \n",
      " 14  nbjet77   int64  \n",
      " 15  nbjet80   int64  \n",
      " 16  isSF      int64  \n",
      " 17  isOS      int64  \n",
      " 18  weight    float64\n",
      " 19  category  object \n",
      " 20  isSignal  int64  \n",
      "dtypes: float64(11), int64(9), object(1)\n",
      "memory usage: 18.0+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "marine-strap",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/software/easybuild/software/SciPy-bundle/2020.11-fosscuda-2020b/lib/python3.8/site-packages/pandas/core/frame.py:4167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "categories = df[\"category\"]\n",
    "\n",
    "background_categories = df[df[\"isSignal\"] == 0][\"category\"].unique()\n",
    "signal_df = df[df['category'] == 'SUSYC1C1']\n",
    "\n",
    "background_df = df[df[\"isSignal\"] == 0]\n",
    "\n",
    "columns_to_drop = [\"category\", \"isSignal\"]\n",
    "\n",
    "\n",
    "signal_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "background_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "signal_mc = signal_df#.to_numpy()\n",
    "background_mc = background_df#.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "pretty-court",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106898318, 19)\n",
      "(135127, 19)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(background_mc))\n",
    "print(np.shape(signal_mc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-ferry",
   "metadata": {},
   "source": [
    "### Data handling and preperations\n",
    "Before we train on the data, we need to scale it and split it into a validation and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "foster-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "subsequent-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split background\n",
    "X_b_train, X_b_val = train_test_split(background_mc, test_size=0.2, random_state=seed)\n",
    "# Split signal\n",
    "#X_s_train, X_s_test = train_test_split(signal_mc, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-statistics",
   "metadata": {},
   "source": [
    "Now, combine samples for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "bound-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train_weights = X_b_train[\"weight\"]\n",
    "b_val_weights = X_b_val[\"weight\"]\n",
    "s_test_weights = signal_mc[\"weight\"]\n",
    "\n",
    "\n",
    "X_b_train.pop(\"weight\")\n",
    "X_b_val.pop(\"weight\")\n",
    "signal_mc.pop(\"weight\")\n",
    "\n",
    "\n",
    "X_s_test = signal_mc\n",
    "\n",
    "\n",
    "X_test = np.concatenate((X_b_val,X_s_test),0)\n",
    "\n",
    "y_b_val = np.zeros(X_b_val.shape[0])                                                                                                                                                                                                                                                   \n",
    "y_s_test = np.ones(X_s_test.shape[0])      \n",
    "y_test = np.concatenate((y_b_val,y_s_test),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "casual-static",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 85518654 entries, 42061528 to 14927437\n",
      "Data columns (total 18 columns):\n",
      " #   Column    Dtype  \n",
      "---  ------    -----  \n",
      " 0   lep_pt1   float64\n",
      " 1   lep_eta1  float64\n",
      " 2   lep_phi1  float64\n",
      " 3   lep_E1    float64\n",
      " 4   lep_pt2   float64\n",
      " 5   lep_eta2  float64\n",
      " 6   lep_phi2  float64\n",
      " 7   lep_E2    float64\n",
      " 8   met       float64\n",
      " 9   mll       float64\n",
      " 10  njet20    int64  \n",
      " 11  njet60    int64  \n",
      " 12  nbjet60   int64  \n",
      " 13  nbjet70   int64  \n",
      " 14  nbjet77   int64  \n",
      " 15  nbjet80   int64  \n",
      " 16  isSF      int64  \n",
      " 17  isOS      int64  \n",
      "dtypes: float64(10), int64(8)\n",
      "memory usage: 12.1 GB\n"
     ]
    }
   ],
   "source": [
    "X_b_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "independent-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_ae = StandardScaler()#MinMaxScaler()\n",
    "X_b_train = scaler_ae.fit_transform(X_b_train)                                                                                                                                                                                                                        \n",
    "X_b_val= scaler_ae.transform(X_b_val)                                                                                                                                                                                                                                 \n",
    "X_s_test = scaler_ae.transform(X_s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "meaningful-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape = np.shape(X_b_train)[1]\n",
    "number_of_rows = np.shape(X_b_train)[0]\n",
    "n_vali = np.shape(X_b_val)[0]\n",
    "\n",
    "random_indices_b = np.random.choice(number_of_rows, size=int(1e6), replace=False)\n",
    "test_indices_b = np.random.choice(n_vali, size=int(200000), replace=False)\n",
    "\n",
    "smaller_data = X_b_train[random_indices_b, :]\n",
    "small_vali = X_b_val[test_indices_b, :]\n",
    "\n",
    "\n",
    "test_indices_sb = np.random.choice(np.shape(X_test)[0], size=int(200000), replace=False)\n",
    "X_small_test = X_test[test_indices_sb, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-stadium",
   "metadata": {},
   "source": [
    "### Training\n",
    "Now we can train on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sunrise-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gridautoencoder(X_b, X_back_test):\n",
    "    tuner = kt.Hyperband(\n",
    "        AE_model_builder,\n",
    "        objective=kt.Objective(\"val_mse\", direction=\"min\"),\n",
    "        max_epochs=50,\n",
    "        factor=3,\n",
    "        directory=\"GridSearches\",\n",
    "        project_name=\"AE\",\n",
    "        overwrite=True,\n",
    "    )\n",
    "\n",
    "    tuner.search(X_b, X_b, epochs=50, batch_size=4000,\n",
    "                 validation_data=(X_back_test, X_back_test))\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "    For Encoder: \\n \n",
    "    First layer has {best_hps.get('num_of_neurons0')} with activation {best_hps.get('0_act')} \\n\n",
    "    Second layer has {best_hps.get('num_of_neurons1')} with activation {best_hps.get('1_act')} \\n\n",
    "    \n",
    "    Latent layer has {best_hps.get(\"lat_num\")} with activation {best_hps.get('2_act')} \\n\n",
    "    \\n\n",
    "    For Decoder: \\n \n",
    "    First layer has {best_hps.get('num_of_neurons5')} with activation {best_hps.get('5_act')}\\n\n",
    "    Second layer has {best_hps.get('num_of_neurons6')} with activation {best_hps.get('6_act')}\\n\n",
    "    Third layer has activation {best_hps.get('7_act')}\\n\n",
    "    \\n\n",
    "    with learning rate = {best_hps.get('learning_rate')} and alpha = {best_hps.get('alpha')}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    state = True\n",
    "    while state == True:\n",
    "        answ = input(\"Do you want to save model? (y/n) \")\n",
    "        if answ == \"y\":\n",
    "            name = input(\"name: \")\n",
    "            tuner.hypermodel.build(best_hps).save(\n",
    "                f\"../tf_models/model_{name}.h5\")\n",
    "            state = False\n",
    "            print(\"Model saved\")\n",
    "        elif answ == \"n\":\n",
    "            state = False\n",
    "            print(\"Model not saved\")\n",
    "\n",
    "\n",
    "def AE_model_builder(hp):\n",
    "    \n",
    "\n",
    "    alpha_choice = hp.Choice(\"alpha\", values=[1., 0.5, 0.1, 0.05, 0.01])\n",
    "    #get_custom_objects().update({\"leakyrelu\": tf.keras.layers.LeakyReLU(alpha=alpha_choice)})\n",
    "    activations = {\n",
    "        \"relu\": tf.nn.relu,\n",
    "        \"tanh\": tf.nn.tanh,\n",
    "        \"leakyrelu\": lambda x: tf.nn.leaky_relu(x, alpha=alpha_choice),\n",
    "        \"linear\": tf.keras.activations.linear\n",
    "    }\n",
    "    inputs = tf.keras.layers.Input(shape=data_shape, name=\"encoder_input\")\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=hp.Int(\"num_of_neurons0\", min_value=13, max_value=17, step=1),\n",
    "        activation=activations.get(hp.Choice(\n",
    "            \"0_act\", [\"relu\", \"tanh\", \"leakyrelu\"])))(inputs)\n",
    "    x1 = tf.keras.layers.Dense(\n",
    "        units=hp.Int(\"num_of_neurons1\", min_value=7, max_value=12, step=1),\n",
    "        activation=activations.get(hp.Choice(\n",
    "            \"1_act\", [\"relu\", \"tanh\", \"leakyrelu\",\"linear\"]))\n",
    "    )(x)\n",
    "    val = hp.Int(\"lat_num\", min_value=1, max_value=6, step=1)\n",
    "    x2 = tf.keras.layers.Dense(\n",
    "        units=val, activation=activations.get(hp.Choice(\n",
    "            \"2_act\", [\"relu\", \"tanh\", \"leakyrelu\",\"linear\"]))\n",
    "    )(x1)\n",
    "    encoder = tf.keras.Model(inputs, x2, name=\"encoder\")\n",
    "\n",
    "    latent_input = tf.keras.layers.Input(shape=val, name=\"decoder_input\")\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=hp.Int(\"num_of_neurons5\", min_value=7, max_value=12, step=1),\n",
    "        activation=activations.get(hp.Choice(\n",
    "            \"5_act\", [\"relu\", \"tanh\", \"leakyrelu\",\"linear\"]))\n",
    "    )(latent_input)\n",
    "    x1 = tf.keras.layers.Dense(\n",
    "        units=hp.Int(\"num_of_neurons6\", min_value=13, max_value=17, step=1),\n",
    "        activation=activations.get(hp.Choice(\n",
    "            \"6_act\", [\"relu\", \"tanh\", \"leakyrelu\",\"linear\"]))\n",
    "    )(x)\n",
    "    output = tf.keras.layers.Dense(\n",
    "        data_shape, activation=activations.get(hp.Choice(\n",
    "            \"7_act\", [\"relu\", \"tanh\", \"leakyrelu\",\"linear\"]))\n",
    "    )(x1)\n",
    "    decoder = tf.keras.Model(latent_input, output, name=\"decoder\")\n",
    "\n",
    "    outputs = decoder(encoder(inputs))\n",
    "    AE_model = tf.keras.Model(inputs, outputs, name=\"AE_model\")\n",
    "\n",
    "    hp_learning_rate = hp.Choice(\"learning_rate\", values=[\n",
    "                                 9e-2, 9.5e-2, 1e-3, 1.5e-3])\n",
    "    optimizer = tf.keras.optimizers.Adam(hp_learning_rate)\n",
    "    AE_model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mse\"])\n",
    "\n",
    "    return AE_model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "retired-slide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-labor",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "retained-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "norman-soviet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 05m 34s]\n",
      "val_mse: 0.4847283959388733\n",
      "\n",
      "Best val_mse So Far: 0.10210715979337692\n",
      "Total elapsed time: 01h 22m 36s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "    For Encoder: \n",
      " \n",
      "    First layer has 16 with activation leakyrelu \n",
      "\n",
      "    Second layer has 7 with activation tanh \n",
      "\n",
      "    \n",
      "    Latent layer has 6 with activation leakyrelu \n",
      "\n",
      "    \n",
      "\n",
      "    For Decoder: \n",
      " \n",
      "    First layer has 10 with activation linear\n",
      "\n",
      "    Second layer has 14 with activation leakyrelu\n",
      "\n",
      "    Third layer has activation linear\n",
      "\n",
      "    \n",
      "\n",
      "    with learning rate = 0.0015 and alpha = 0.01\n",
      "    \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save model? (y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not saved\n"
     ]
    }
   ],
   "source": [
    "#with tf.device(\"/CPU:0\"):\n",
    "gridautoencoder(smaller_data, small_vali)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "hundred-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypermodel = tf.keras.models.load_model(\"../tf_models/model_prelim_ae_2lep_data.h5\")\n",
    "inputs = tf.keras.layers.Input(shape=data_shape, name=\"encoder_input\")\n",
    "x = tf.keras.layers.Dense(units=16,activation=tf.keras.layers.LeakyReLU(alpha=0.01))(inputs)\n",
    "x1 = tf.keras.layers.Dense(units=6,activation=\"tanh\")(x)\n",
    "val = 6\n",
    "x2 = tf.keras.layers.Dense(units=val, activation=tf.keras.layers.LeakyReLU(alpha=0.01))(x1)\n",
    "encoder = tf.keras.Model(inputs, x2, name=\"encoder\")\n",
    "\n",
    "latent_input = tf.keras.layers.Input(shape=val, name=\"decoder_input\")\n",
    "x = tf.keras.layers.Dense(units=10,activation=\"linear\")(latent_input)\n",
    "x1 = tf.keras.layers.Dense(units=14,activation=tf.keras.layers.LeakyReLU(alpha=0.01))(x)\n",
    "output = tf.keras.layers.Dense(data_shape, activation=\"linear\")(x1)\n",
    "decoder = tf.keras.Model(latent_input, output, name=\"decoder\")\n",
    "\n",
    "outputs = decoder(encoder(inputs))\n",
    "AE_model = tf.keras.Model(inputs, outputs, name=\"AE_model\")\n",
    "\n",
    "hp_learning_rate = 0.0015\n",
    "optimizer = tf.keras.optimizers.Adam(hp_learning_rate)\n",
    "AE_model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-earth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  873/21380 [>.............................] - ETA: 6:11 - loss: 0.2915 - mse: 0.2915"
     ]
    }
   ],
   "source": [
    "#with tf.device(\"/CPU:0\"):\n",
    "AE_model.fit(X_b_train, X_b_train, epochs=1, batch_size=4000, validation_data=(X_b_val, X_b_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate prediction\n",
    "#with tf.device(\"/CPU:0\"):\n",
    "pred_back = AE_model.predict(X_b_val)\n",
    "print(\"Background done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sig = AE_model.predict(X_s_test)\n",
    "print(\"Signal done\")\n",
    "#pred_data = AE_model.predict(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-sustainability",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "Now we implement testing of the data, and stacking of histograms with the reconstruction <br> for the given background processes, a signal, and ATLAS data. <br>\n",
    "<br>\n",
    "First for background\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from func import reconstructionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_err_back = reconstructionError(pred_back, X_b_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-receiver",
   "metadata": {},
   "source": [
    "Then signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_err_sig = reconstructionError(pred_sig, X_s_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-painting",
   "metadata": {},
   "source": [
    "An then for actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recon_data = tf.keras.losses.msle(pred_data, X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(recon_err_back), np.min(recon_err_back), np.max(recon_err_back))\n",
    "print(np.shape(recon_err_sig), np.min(recon_err_sig), np.max(recon_err_sig))\n",
    "#print(np.shape(recon_data), np.min(recon_data), np.max(recon_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-kelly",
   "metadata": {},
   "source": [
    "Then  plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = recon_err_back#/np.max(recon_err_back)\n",
    "b_s = recon_err_sig#/np.max(recon_err_sig)\n",
    "#norm_recon_data = recon_data/np.max(recon_data)\n",
    "\n",
    "histo_data = [b,b_s]#, norm_recon_data])\n",
    "sample_weight = np.concatenate((b_val_weights, s_test_weights),0)\n",
    "weight_histo = [b_val_weights, s_test_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plot_set\n",
    "plt.rcParams[\"figure.figsize\"] = (12,9)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "n_bins= 200\n",
    "colors = [\"green\", \"red\"]\n",
    "labels= [\"Background\", \"Signal\"]\n",
    "ax.hist(histo_data, n_bins, density=True, stacked=True, histtype='bar', color=colors, label=labels, weights=(b_val_weights, s_test_weights))\n",
    "ax.legend(prop={'size': 10})\n",
    "ax.set_title('Reconstruction error histogram with background and signal')\n",
    "ax.set_xlabel('log10 Reconstruction error')\n",
    "ax.set_ylabel('#Events')\n",
    "#ax.set_yscale('log')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"b_s_recon.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-renewal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rental-dream",
   "metadata": {},
   "source": [
    "Here we plot the ROC curves for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "Y_b = np.zeros(X_b_val.shape[0]).reshape(X_b_val.shape[0],1);\n",
    "Y_s = np.ones(X_s_test.shape[0]).reshape(X_s_test.shape[0],1);\n",
    "Y_ROC = np.concatenate((Y_b,Y_s),0);\n",
    "\n",
    "print(b_val_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rec_errors_ROC = np.concatenate((recon_err_back,recon_err_sig),0)\n",
    "\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_ROC, rec_errors_ROC, sample_weight = sample_weight, pos_label=1)\n",
    "ae_auc = roc_auc_score(Y_ROC, rec_errors_ROC)\n",
    "figRocAE, axRocAE = plt.subplots()\n",
    "figRocAE.set_size_inches(12,12)\n",
    "axRocAE.plot(fpr, tpr, label='ROC curve')\n",
    "axRocAE.plot([0, 1], [0, 1], 'k--')\n",
    "axRocAE.set_xlim([0.0, 1.0])\n",
    "axRocAE.set_ylim([0.0, 1.05])\n",
    "axRocAE.set_xlabel('False Anomaly Rate')\n",
    "axRocAE.set_ylabel('True Anomaly Rate')\n",
    "axRocAE.text(0.4,0.2,\"AUC = %.4f\" % ae_auc,fontsize=15)\n",
    "axRocAE.set_title(\"Autoencoder ROC\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-morrison",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
