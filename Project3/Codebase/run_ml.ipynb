{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protecting-carnival",
   "metadata": {},
   "source": [
    "# Deplyment of semi unsupervised learning on ATLAS OpenData\n",
    "### Testing of reading in data and trying an auto encoder\n",
    "Remember to pip3 install keras-tuner to tune for the given session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coupled-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "seed = tf.random.set_seed(1)\n",
    "#import ROOT as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atlas = pd.read_hdf(\"/storage/shared/data/2lep_df_forML_data_fromRDF.hdf5\")\n",
    "df = pd.read_hdf(\"/storage/shared/data/2lep_df_forML_bkg_signal_fromRDF.hdf5\")\n",
    "\n",
    "#weights = pd.read_csv(\"weights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df_atlas.info()\n",
    "#weights.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-individual",
   "metadata": {},
   "source": [
    "### Adding more columns \n",
    "Because the dataset is lacking the \"isSignal\" and \"weight\" columns, we have to create them our self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = ['ZPrimemumu', 'SUSYC1C1', 'RS_G_ZZ', 'SUSYSlepSlep', 'SUSYC1N2', 'ZPrimett', 'ZPrimeee', 'dmV_Zll', 'GG_ttn1', 'TT_directTT', 'Gee', 'Gmumu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "issignal = np.where(df[\"category\"].isin(signals)  , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(isSignal=issignal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atlas.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-victory",
   "metadata": {},
   "source": [
    "### Data separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df[\"category\"]\n",
    "\n",
    "background_categories = df[df[\"isSignal\"] == 0][\"category\"].unique()\n",
    "signal_df = df[df['category'] == 'SUSYC1C1']\n",
    "\n",
    "background_df = df[df[\"isSignal\"] == 0]\n",
    "\n",
    "columns_to_drop = [\"category\", \"isSignal\", \"physdescr\"]\n",
    "\n",
    "\n",
    "signal_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "background_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "signal_mc = signal_df#.to_numpy()\n",
    "background_mc = background_df#.to_numpy()\n",
    "\n",
    "data = df_atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(background_mc))\n",
    "print(np.shape(signal_mc))\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-heavy",
   "metadata": {},
   "source": [
    "### Data handling and preperations\n",
    "Before we train on the data, we need to scale it and split it into a validation and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split background\n",
    "X_b_train, X_b_val = train_test_split(background_mc, test_size=0.2, random_state=seed)\n",
    "# Split signal\n",
    "#X_s_train, X_s_test = train_test_split(signal_mc, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-litigation",
   "metadata": {},
   "source": [
    "Now, combine samples for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train_weights = X_b_train[\"weight\"]\n",
    "b_val_weights = X_b_val[\"weight\"]\n",
    "s_test_weights = signal_mc[\"weight\"]\n",
    "\n",
    "data_weights = data[\"weight\"]\n",
    "\n",
    "X_b_train.pop(\"weight\")\n",
    "X_b_val.pop(\"weight\")\n",
    "signal_mc.pop(\"weight\")\n",
    "data.pop(\"weight\")\n",
    "\n",
    "\n",
    "X_s_test = signal_mc\n",
    "\n",
    "\n",
    "X_test = np.concatenate((X_b_val,X_s_test),0)\n",
    "\n",
    "y_b_val = np.zeros(X_b_val.shape[0])                                                                                                                                                                                                                                                   \n",
    "y_s_test = np.ones(X_s_test.shape[0])      \n",
    "y_test = np.concatenate((y_b_val,y_s_test),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_b_train))\n",
    "print(np.shape(X_s_test))\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_ae = StandardScaler()#MinMaxScaler()\n",
    "X_b_train = scaler_ae.fit_transform(X_b_train)                                                                                                                                                                                                                        \n",
    "X_b_val= scaler_ae.transform(X_b_val)                                                                                                                                                                                                                                 \n",
    "X_s_test = scaler_ae.transform(X_s_test)\n",
    "data = scaler_ae.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape = np.shape(X_b_train)[1]\n",
    "number_of_rows = np.shape(X_b_train)[0]\n",
    "n_vali = np.shape(X_b_val)[0]\n",
    "\n",
    "random_indices_b = np.random.choice(number_of_rows, size=int(1e6), replace=False)\n",
    "test_indices_b = np.random.choice(n_vali, size=int(200000), replace=False)\n",
    "\n",
    "smaller_data = X_b_train[random_indices_b, :]\n",
    "small_vali = X_b_val[test_indices_b, :]\n",
    "\n",
    "\n",
    "test_indices_sb = np.random.choice(np.shape(X_test)[0], size=int(200000), replace=False)\n",
    "X_small_test = X_test[test_indices_sb, :]\n",
    "\n",
    "in_and_out = X_b_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-preference",
   "metadata": {},
   "source": [
    "Try to load data faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-personality",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-grocery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "naval-camel",
   "metadata": {},
   "source": [
    "### Training\n",
    "Now we can train on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gridautoencoder(X_b, X_back_test):\n",
    "    tuner = kt.Hyperband(\n",
    "        AE_model_builder,\n",
    "        objective=kt.Objective(\"val_mse\", direction=\"min\"),\n",
    "        max_epochs=50,\n",
    "        factor=3,\n",
    "        directory=\"GridSearches\",\n",
    "        project_name=\"AE\",\n",
    "        overwrite=True,\n",
    "    )\n",
    "\n",
    "    tuner.search(X_b.as_numpy_iterator(), X_b.as_numpy_iterator(), epochs=50, batch_size=512,\n",
    "                 validation_data=(X_back_test.as_numpy_iterator(), X_back_test.as_numpy_iterator()))\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "    For Encoder: \\n \n",
    "    First layer has {best_hps.get('num_of_neurons0')} with activation {best_hps.get('0_act')} \\n\n",
    "    Second layer has {best_hps.get('num_of_neurons1')} with activation {best_hps.get('1_act')} \\n\n",
    "    \n",
    "    Latent layer has {best_hps.get(\"lat_num\")} with activation {best_hps.get('2_act')} \\n\n",
    "    \\n\n",
    "    For Decoder: \\n \n",
    "    First layer has {best_hps.get('num_of_neurons5')} with activation {best_hps.get('5_act')}\\n\n",
    "    Second layer has {best_hps.get('num_of_neurons6')} with activation {best_hps.get('6_act')}\\n\n",
    "    Third layer has activation {best_hps.get('7_act')}\\n\n",
    "    \\n\n",
    "    with learning rate = {best_hps.get('learning_rate')} and alpha = {best_hps.get('alpha')}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    state = True\n",
    "    while state == True:\n",
    "        answ = input(\"Do you want to save model? (y/n) \")\n",
    "        if answ == \"y\":\n",
    "            name = input(\"name: \")\n",
    "            tuner.hypermodel.build(best_hps).save(\n",
    "                f\"../tf_models/model_{name}.h5\")\n",
    "            state = False\n",
    "            print(\"Model saved\")\n",
    "        elif answ == \"n\":\n",
    "            state = False\n",
    "            print(\"Model not saved\")\n",
    "\n",
    "\n",
    "def AE_model_builder(hp):\n",
    "    \n",
    "\n",
    "    alpha_choice = hp.Choice(\"alpha\", values=[1., 0.5, 0.1, 0.05, 0.01])\n",
    "    #get_custom_objects().update({\"leakyrelu\": tf.keras.layers.LeakyReLU(alpha=alpha_choice)})\n",
    "    activations = {\n",
    "        \"relu\": tf.nn.relu,\n",
    "        \"tanh\": tf.nn.tanh,\n",
    "        \"leakyrelu\": lambda x: tf.nn.leaky_relu(x, alpha=alpha_choice),\n",
    "        \"linear\": tf.keras.activations.linear\n",
    "    }\n",
    "    inputs = tf.keras.layers.Input(shape=data_shape, name=\"encoder_input\")\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=hp.Int(\"num_of_neurons0\", min_value=16, max_value=data_shape-1, step=1),\n",
    "        activation=activations.get(hp.Choice(\n",
    "            \"0_act\", [\"relu\", \"tanh\", \"leakyrelu\"])))(inputs)\n",
    "    x1 = tf.keras.layers.Dense(\n",
    "        units=hp.Int(\"num_of_neurons1\", min_value=7, max_value=15, step=1),\n",
    "        activation=activations.get(hp.Choice(\n",
    "            \"1_act\", [\"relu\", \"tanh\", \"leakyrelu\",\"linear\"]))\n",
    "    )(x)\n",
    "    val = hp.Int(\"lat_num\", min_value=1, max_value=6, step=1)\n",
    "    x2 = tf.keras.layers.Dense(\n",
    "        units=val, activation=activations.get(hp.Choice(\n",
    "            \"2_act\", [\"relu\", \"tanh\", \"leakyrelu\",\"linear\"]))\n",
    "    )(x1)\n",
    "    encoder = tf.keras.Model(inputs, x2, name=\"encoder\")\n",
    "\n",
    "    latent_input = tf.keras.layers.Input(shape=val, name=\"decoder_input\")\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=hp.Int(\"num_of_neurons5\", min_value=7, max_value=15, step=1),\n",
    "        activation=activations.get(hp.Choice(\n",
    "            \"5_act\", [\"relu\", \"tanh\", \"leakyrelu\",\"linear\"]))\n",
    "    )(latent_input)\n",
    "    x1 = tf.keras.layers.Dense(\n",
    "        units=hp.Int(\"num_of_neurons6\", min_value=16, max_value=data_shape-1, step=1),\n",
    "        activation=activations.get(hp.Choice(\n",
    "            \"6_act\", [\"relu\", \"tanh\", \"leakyrelu\",\"linear\"]))\n",
    "    )(x)\n",
    "    output = tf.keras.layers.Dense(\n",
    "        data_shape, activation=activations.get(hp.Choice(\n",
    "            \"7_act\", [\"relu\", \"tanh\", \"leakyrelu\",\"linear\"]))\n",
    "    )(x1)\n",
    "    decoder = tf.keras.Model(latent_input, output, name=\"decoder\")\n",
    "\n",
    "    outputs = decoder(encoder(inputs))\n",
    "    AE_model = tf.keras.Model(inputs, outputs, name=\"AE_model\")\n",
    "\n",
    "    hp_learning_rate = hp.Choice(\"learning_rate\", values=[\n",
    "                                 9e-2, 9.5e-2, 1e-3, 1.5e-3])\n",
    "    optimizer = tf.keras.optimizers.Adam(hp_learning_rate)\n",
    "    #optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "    AE_model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mse\"])\n",
    "\n",
    "    return AE_model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    gridautoencoder(smaller_data, small_vali)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-import",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-cooler",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypermodel = tf.keras.models.load_model(\"../tf_models/model_prelim_ae_2lep_data.h5\")\n",
    "inputs = tf.keras.layers.Input(shape=data_shape, name=\"encoder_input\")\n",
    "x = tf.keras.layers.Dense(units=16,activation=tf.keras.layers.LeakyReLU(alpha=0.01))(inputs)\n",
    "x1 = tf.keras.layers.Dense(units=6,activation=\"tanh\")(x)\n",
    "val = 6\n",
    "x2 = tf.keras.layers.Dense(units=val, activation=tf.keras.layers.LeakyReLU(alpha=0.01))(x1)\n",
    "encoder = tf.keras.Model(inputs, x2, name=\"encoder\")\n",
    "\n",
    "latent_input = tf.keras.layers.Input(shape=val, name=\"decoder_input\")\n",
    "x = tf.keras.layers.Dense(units=10,activation=\"linear\")(latent_input)\n",
    "x1 = tf.keras.layers.Dense(units=14,activation=tf.keras.layers.LeakyReLU(alpha=0.01))(x)\n",
    "output = tf.keras.layers.Dense(data_shape, activation=\"linear\")(x1)\n",
    "decoder = tf.keras.Model(latent_input, output, name=\"decoder\")\n",
    "\n",
    "outputs = decoder(encoder(inputs))\n",
    "AE_model = tf.keras.Model(inputs, outputs, name=\"AE_model\")\n",
    "\n",
    "hp_learning_rate = 0.0015\n",
    "optimizer = tf.keras.optimizers.Adam(hp_learning_rate)\n",
    "AE_model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    AE_model.fit(X_b_train, X_b_train, epochs=2, batch_size=4000, validation_data=(X_b_val, X_b_val), use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate prediction\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    pred_back = AE_model.predict(X_b_val)\n",
    "    print(\"Background done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    pred_sig = AE_model.predict(X_s_test)\n",
    "    print(\"Signal done\")\n",
    "#pred_data = AE_model.predict(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    pred_data = AE_model.predict(data)\n",
    "    print(\"ATLAS data done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-aberdeen",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "Now we implement testing of the data, and stacking of histograms with the reconstruction <br> for the given background processes, a signal, and ATLAS data. <br>\n",
    "<br>\n",
    "First for background\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "from func import reconstructionError\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_err_back = reconstructionError(pred_back, X_b_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-credit",
   "metadata": {},
   "source": [
    "Then signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_err_sig = reconstructionError(pred_sig, X_s_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-cisco",
   "metadata": {},
   "source": [
    "An then for actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recon_data = tf.keras.losses.msle(pred_data, X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(recon_err_back), np.min(recon_err_back), np.max(recon_err_back))\n",
    "print(np.shape(recon_err_sig), np.min(recon_err_sig), np.max(recon_err_sig))\n",
    "#print(np.shape(recon_data), np.min(recon_data), np.max(recon_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-script",
   "metadata": {},
   "source": [
    "Then  plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = recon_err_back#/np.max(recon_err_back)\n",
    "b_s = recon_err_sig#/np.max(recon_err_sig)\n",
    "#norm_recon_data = recon_data/np.max(recon_data)\n",
    "\n",
    "histo_data = [b_s,b]#, norm_recon_data])\n",
    "weight_histo = [s_test_weights,b_val_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plot_set\n",
    "plt.rcParams[\"figure.figsize\"] = (12,9)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "n_bins= 200\n",
    "colors = [\"green\", \"red\"]\n",
    "labels= [\"Signal\", \"Background\"]\n",
    "ax.hist(histo_data, n_bins, density=True, stacked=True, histtype='bar', color=colors, label=labels, weights=(s_test_weights, b_val_weights))\n",
    "ax.legend(prop={'size': 10})\n",
    "ax.set_title('Reconstruction error histogram with background and signal')\n",
    "ax.set_xlabel('log10 Reconstruction error')\n",
    "ax.set_ylabel('#Events')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"b_s_recon.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-drinking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "treated-japanese",
   "metadata": {},
   "source": [
    "Here we plot the ROC curves for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "Y_b = np.zeros(X_b_val.shape[0]).reshape(X_b_val.shape[0],1);\n",
    "Y_s = np.ones(X_s_test.shape[0]).reshape(X_s_test.shape[0],1);\n",
    "Y_ROC = np.concatenate((Y_s, Y_b),0);\n",
    "\n",
    "sample_weight = np.concatenate((s_test_weights, b_val_weights),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rec_errors_ROC = np.concatenate((recon_err_sig,recon_err_back),0)\n",
    "\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_ROC, rec_errors_ROC, sample_weight = sample_weight, pos_label=1)\n",
    "ae_auc = roc_auc_score(Y_ROC, rec_errors_ROC)\n",
    "figRocAE, axRocAE = plt.subplots()\n",
    "figRocAE.set_size_inches(12,12)\n",
    "axRocAE.plot(fpr, tpr, label='ROC curve')\n",
    "axRocAE.plot([0, 1], [0, 1], 'k--')\n",
    "axRocAE.set_xlim([0.0, 1.0])\n",
    "axRocAE.set_ylim([0.0, 1.05])\n",
    "axRocAE.set_xlabel('False Anomaly Rate')\n",
    "axRocAE.set_ylabel('True Anomaly Rate')\n",
    "axRocAE.text(0.4,0.2,\"AUC = %.4f\" % ae_auc,fontsize=15)\n",
    "axRocAE.set_title(\"Autoencoder ROC\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"b_s_roc_curve.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-smith",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-speaker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-catering",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
