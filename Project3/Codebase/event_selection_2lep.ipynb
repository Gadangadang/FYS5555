{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When running this notebook via the Galaxy portal\n",
    "You can access your data via the dataset number. Using a Python kernel, you can access dataset number 42 with ``handle = open(get(42), 'r')``.\n",
    "To save data, write your data to a file, and then call ``put('filename.txt')``. The dataset will then be available in your galaxy history.\n",
    "<br><br>Note that if you are putting/getting to/from a different history than your default history, you must also provide the history-id.\n",
    "<br><br>More information including available galaxy-related environment variables can be found at https://github.com/bgruening/docker-jupyter-notebook. This notebook is running in a docker container based on the Docker Jupyter container described in that link.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling (Python) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT as R\n",
    "import import_ipynb\n",
    "import setPath\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from Input.OpenDataPandaFramework13TeV import *\n",
    "%jsroot on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the analaysis to run (*1largeRjet1lep*, *1lep1tau*, *3lep*, *exactly2lep*, *GamGam*, *2lep*, *4lep*)\n",
    "\n",
    "Set the directory where you have downloaded the ATLAS OpenData samples you want to run over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opendatadir = \"/storage/shared/data/fys5555/ATLAS_opendata/\"\n",
    "analysis = \"2lep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = R.TChain(\"mini\")\n",
    "data = R.TChain(\"mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of all the background samples, category and their IDs can be found in **Infofile.txt**. The cross-section, efficiencies etc. needed for scaling are stored in the **Files_<---->**. We read these files and add all the samples to the TChain. We also (for later convenience) make a vector containing the dataset IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcfiles = initialize(opendatadir+\"/\"+analysis+\"/MC\")\n",
    "datafiles = initialize(opendatadir+\"/\"+analysis+\"/Data\")\n",
    "allfiles = z = {**mcfiles, **datafiles}\n",
    "Backgrounds = getBkgCategories() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCcat = {}\n",
    "for cat in allfiles:\n",
    "    for dsid in allfiles[cat][\"dsid\"]:\n",
    "        try:\n",
    "            MCcat[int(dsid)] = cat\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_IDs = []\n",
    "background.Reset()\n",
    "for b in Backgrounds:\n",
    "    i = 0\n",
    "    if not b in mcfiles.keys(): continue\n",
    "    for mc in mcfiles[b][\"files\"]:\n",
    "        if not os.path.isfile(mc): continue\n",
    "        try:\n",
    "            dataset_IDs.append(int(mcfiles[b][\"dsid\"][i]))\n",
    "            background.Add(mc)\n",
    "        except:\n",
    "            print(\"Could not get DSID for %s. Skipping\"%mc)\n",
    "        i += 1\n",
    "nen = background.GetEntries()\n",
    "print(\"Added %i entries for backgrounds\"%(nen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Reset(); \n",
    "for d in datafiles[\"data\"][\"files\"]:  \n",
    "    if not os.path.isfile(d): continue\n",
    "    data.Add(d)\n",
    "nen = data.GetEntries()\n",
    "print(\"Added %i entries for data\"%(nen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Event selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning using (semi)-unsupervised learning, we need as unbiased data as possible. Thus, we need to construct a dataframe with all the neccesary features, which is a lot of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve lumi from library\n",
    "%store -r lumi\n",
    "\n",
    "l1 = R.TLorentzVector()\n",
    "l2 = R.TLorentzVector()\n",
    "\n",
    "dilepton = R.TLorentzVector()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cell where the analysis is performed. Note that the cell needs to be run twice:\n",
    "\n",
    "1. with data = 0 to run over MC\n",
    "2. with data = 1 to run over data\n",
    "\n",
    "Note that the MC running takes ~5 minutes for 3lep analysis. Much(!!!) more time for e.g. 2lep analysis! Data running is relatively fast for 3lep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "isData = 0; \n",
    "\n",
    "if isData == 1: ds = data \n",
    "else: ds = background\n",
    "\n",
    "legal_flavor_tot = [13*4, 11*4]\n",
    "    \n",
    "columns = {\"met\":[], \"XSection\":[], \n",
    "           \"lep_n\":[],\"tot_lep_invariant_mass\":[], \"mean_lep_pt\":[], \n",
    "           \"mean_lep_E\":[], \"mean_lep_ptcone30\":[], \"mean_lep_etcone20\":[],\n",
    "           \"mean_lep_eta\":[], \"mean_lep_phi\":[], \"jet_n\":[], \n",
    "           \"mean_jet_pt\":[], \"mean_jet_E\":[], \"mean_jet_eta\":[], \n",
    "           \"mean_jet_phi\":[], \"photon_n\":[], \"mean_photon_pt\":[], \n",
    "           \"mean_photon_E\":[], \"mean_photon_ptcone30\":[], \n",
    "           \"mean_photon_etcone20\":[],\"mean_photon_eta\":[], \n",
    "           \"mean_photon_phi\":[], \"largeRjet_n\":[], \"tot_largeRjet_m\":[],\n",
    "           \"mean_largeRjet_pt\":[], \"mean_largeRjet_E\":[], \n",
    "           \"mean_largeRjet_eta\":[], \"mean_largeRjet_phi\":[],\n",
    "           \"tau_n\":[], \"mean_tau_pt\":[], \"mean_tau_E\":[], \n",
    "           \"mean_tau_eta\":[], \"mean_tau_phi\":[],\n",
    "           \"mean_lep_pt_syst\":[], \"met_et_syst\":[], \n",
    "           \"mean_jet_pt_syst\":[], \"mean_photon_pt_syst\":[], \n",
    "           \"mean_largeRjet_pt_syst\":[], \"mean_tau_pt_syst\":[]\n",
    "          }\n",
    "\n",
    "i = 0   \n",
    "for event in ds: \n",
    "   \n",
    "    # First event selection, require charge conservation, and lep flavor, and \n",
    "    # to only have the two largest leptons    \n",
    "    \n",
    "    ## Cut #1: Require 2 or more leptons, but must find the indices of the two we pick\n",
    "    if not ds.lep_n >= 2: continue\n",
    "\n",
    "    lep_pt = np.zeros(ds.lep_n)\n",
    "  \n",
    "    lep_type = np.zeros(ds.lep_n)\n",
    "    \n",
    "    for j in range(ds.lep_n):\n",
    "        lep_pt[j] = ds.lep_pt[j]\n",
    "        lep_type[j] = ds.lep_type[j]\n",
    "        \n",
    "    \n",
    "    \n",
    "    elec_index = np.where(lep_type == 11)[0]\n",
    "    muon_index = np.where(lep_type == 13)[0]\n",
    "    \n",
    "    if (len(muon_index) < len(elec_index)) and (len(elec_index) == 2):\n",
    "        lep_index = elec_index\n",
    "    \n",
    "    elif (len(elec_index) < len(muon_index)) and (len(muon_index) == 2): \n",
    "        lep_index = muon_index\n",
    "        \n",
    "    else:  \n",
    "        max_index = np.where(lep_pt == np.max(lep_pt))[0]\n",
    "        lep_typ = lep_type[max_index]\n",
    "        lep_ind = np.where(lep_type == lep_typ)[0]\n",
    "    \n",
    "    lep0 = int(lep_index[0])\n",
    "    lep1 = int(lep_index[1])\n",
    "    \n",
    "    ## Cut #2: Require opposite charge\n",
    "    if not ds.lep_charge[lep0] + ds.lep_charge[lep1] == 0 : continue\n",
    "    \n",
    "    ## Require \"good leptons\": \n",
    "    \n",
    "    if ds.lep_pt[lep0]/1000.0 < 25: continue\n",
    "    if ds.lep_etcone20[lep0]/ds.lep_pt[lep0] > 0.15:\n",
    "        continue\n",
    "    if ds.lep_ptcone30[lep0]/ds.lep_pt[lep0] > 0.15:\n",
    "        continue\n",
    "    #if not (ds.lep_flag[0] & 512): continue\n",
    "        \n",
    "    if ds.lep_pt[lep1]/1000.0 < 25:\n",
    "        continue\n",
    "    if ds.lep_etcone20[lep1]/ds.lep_pt[lep1] > 0.15:\n",
    "        continue\n",
    "    if ds.lep_ptcone30[lep1]/ds.lep_pt[lep1] > 0.15:\n",
    "        continue\n",
    "    #if not (ds.lep_flag[1] & 512): continue\n",
    "    \n",
    "    l1.SetPtEtaPhiE(ds.lep_pt[lep0]/1000., ds.lep_eta[lep0],\n",
    "                    ds.lep_phi[lep0], ds.lep_E[lep0]/1000.)\n",
    "    l2.SetPtEtaPhiE(ds.lep_pt[lep1]/1000., ds.lep_eta[lep1],\n",
    "                    ds.lep_phi[lep1], ds.lep_E[lep1]/1000.)\n",
    "    \n",
    "    dilepton = l1 + l2 \n",
    "    \n",
    "    dilep_inv_mass = dilepton.M()\n",
    "    ## Event selection:\n",
    "    \n",
    "    ### General information \n",
    "    columns[\"met\"].append(ds.met_et/1000.0)\n",
    "    columns[\"XSection\"].append(ds.XSection)  \n",
    "    \n",
    "    ### Lep information\n",
    "    columns[\"lep_n\"].append(ds.lep_n)\n",
    "    columns[\"tot_lep_invariant_mass\"].append(dilep_inv_mass)\n",
    "    columns[\"mean_lep_pt\"].append(np.mean(ds.lep_pt))\n",
    "    columns[\"mean_lep_E\"].append(np.mean(ds.lep_E))\n",
    "    columns[\"mean_lep_ptcone30\"].append(np.mean(ds.lep_ptcone30))\n",
    "    columns[\"mean_lep_etcone20\"].append(np.mean(ds.lep_etcone20))\n",
    "    columns[\"mean_lep_eta\"].append(np.mean(ds.lep_eta))\n",
    "    columns[\"mean_lep_phi\"].append(np.mean(ds.lep_phi))\n",
    "    \n",
    "    ### Jet information\n",
    "    if ds.jet_n == 0:\n",
    "        columns[\"jet_n\"].append(ds.jet_n)\n",
    "        columns[\"mean_jet_pt\"].append(0)\n",
    "        columns[\"mean_jet_E\"].append(0)\n",
    "        columns[\"mean_jet_eta\"].append(0)\n",
    "        columns[\"mean_jet_phi\"].append(0)\n",
    "        columns[\"mean_jet_pt_syst\"].append(0)\n",
    "    else:\n",
    "        columns[\"jet_n\"].append(ds.jet_n)\n",
    "        columns[\"mean_jet_pt\"].append(np.mean(ds.jet_pt))\n",
    "        columns[\"mean_jet_E\"].append(np.mean(ds.jet_E))\n",
    "        columns[\"mean_jet_eta\"].append(np.mean(ds.jet_eta))\n",
    "        columns[\"mean_jet_phi\"].append(np.mean(ds.jet_phi))\n",
    "        columns[\"mean_jet_pt_syst\"].append(ds.jet_pt_syst)\n",
    "    \n",
    "    ### Photon information\n",
    "    if ds.photon_n == 0:\n",
    "        columns[\"photon_n\"].append(ds.photon_n)\n",
    "        columns[\"mean_photon_pt\"].append(0)\n",
    "        columns[\"mean_photon_E\"].append(0)\n",
    "        columns[\"mean_photon_ptcone30\"].append(0)\n",
    "        columns[\"mean_photon_etcone20\"].append(0)\n",
    "        columns[\"mean_photon_eta\"].append(0)\n",
    "        columns[\"mean_photon_phi\"].append(0)\n",
    "        columns[\"mean_photon_pt_syst\"].append(0)\n",
    "    else:   \n",
    "        columns[\"photon_n\"].append(ds.photon_n)\n",
    "        columns[\"mean_photon_pt\"].append(np.mean(ds.photon_pt))\n",
    "        columns[\"mean_photon_E\"].append(np.mean(ds.photon_E))\n",
    "        columns[\"mean_photon_ptcone30\"].append(np.mean(ds.photon_ptcone30))\n",
    "        columns[\"mean_photon_etcone20\"].append(np.mean(ds.photon_etcone20))\n",
    "        columns[\"mean_photon_eta\"].append(np.mean(ds.photon_eta))\n",
    "        columns[\"mean_photon_phi\"].append(np.mean(ds.photon_phi))\n",
    "        columns[\"mean_photon_pt_syst\"].append(ds.photon_pt_syst)\n",
    "    \n",
    "    ### LargeRjet information\n",
    "    if ds.largeRjet_n == 0 :\n",
    "        columns[\"largeRjet_n\"].append(ds.largeRjet_n)\n",
    "        columns[\"tot_largeRjet_m\"].append(0)\n",
    "        columns[\"mean_largeRjet_pt\"].append(0)\n",
    "        columns[\"mean_largeRjet_E\"].append(0)\n",
    "        columns[\"mean_largeRjet_eta\"].append(0)\n",
    "        columns[\"mean_largeRjet_phi\"].append(0)\n",
    "        columns[\"mean_largeRjet_pt_syst\"].append(0)\n",
    "    else:\n",
    "        columns[\"largeRjet_n\"].append(ds.largeRjet_n)\n",
    "        columns[\"tot_largeRjet_m\"].append(ds.largeRjet_m)\n",
    "        columns[\"mean_largeRjet_pt\"].append(np.mean(ds.largeRjet_pt))\n",
    "        columns[\"mean_largeRjet_E\"].append(np.mean(ds.largeRjet_E))\n",
    "        columns[\"mean_largeRjet_eta\"].append(np.mean(ds.largeRjet_eta))\n",
    "        columns[\"mean_largeRjet_phi\"].append(np.mean(ds.largeRjet_phi))\n",
    "        columns[\"mean_largeRjet_pt_syst\"].append(ds.largeRjet_pt_syst)\n",
    "    \n",
    "    \n",
    "    ### Tau information \n",
    "    if ds.tau_n == 0:\n",
    "        columns[\"tau_n\"].append(ds.tau_n)\n",
    "        columns[\"mean_tau_pt\"].append(0)\n",
    "        columns[\"mean_tau_E\"].append(0)\n",
    "        columns[\"mean_tau_eta\"].append(0)\n",
    "        columns[\"mean_tau_phi\"].append(0)\n",
    "        columns[\"mean_tau_pt_syst\"].append(0)\n",
    "    else:\n",
    "        columns[\"tau_n\"].append(ds.tau_n)\n",
    "        columns[\"mean_tau_pt\"].append(np.mean(ds.tau_pt))\n",
    "        columns[\"mean_tau_E\"].append(np.mean(ds.tau_E))\n",
    "        columns[\"mean_tau_eta\"].append(np.mean(ds.tau_eta))\n",
    "        columns[\"mean_tau_phi\"].append(np.mean(ds.tau_phi))\n",
    "        columns[\"mean_tau_pt_syst\"].append(ds.tau_pt_syst)\n",
    "    \n",
    "    \n",
    "    ### Systematic uncertainty for gauranteed placeholders\n",
    "    columns[\"mean_lep_pt_syst\"].append(ds.lep_pt_syst)\n",
    "    columns[\"met_et_syst\"].append(ds.met_et_syst)\n",
    "    \n",
    "   \n",
    "    if i%10000 == 0 and i>0: \n",
    "        print(\"Total events %i/%i\"%(i,ds.GetEntries()))\n",
    "    i += 1 \n",
    "\n",
    "df = pd.DataFrame(data=columns)\n",
    "        \n",
    "print(\"Done!\")\n",
    "if isData == 0:\n",
    "    df.to_csv(\"mcdata.csv\", \"mini\")\n",
    "    print(\"Remebered to run over data? No? Set data = 1 at the top and run again\")\n",
    "else:\n",
    "    df.to_csv(\"testdata.csv\", \"mini\")\n",
    "    print(\"Remebered to run over MC? No? Set data = 0 at the top and run again\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
