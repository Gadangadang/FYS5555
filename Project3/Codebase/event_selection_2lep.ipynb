{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When running this notebook via the Galaxy portal\n",
    "You can access your data via the dataset number. Using a Python kernel, you can access dataset number 42 with ``handle = open(get(42), 'r')``.\n",
    "To save data, write your data to a file, and then call ``put('filename.txt')``. The dataset will then be available in your galaxy history.\n",
    "<br><br>Note that if you are putting/getting to/from a different history than your default history, you must also provide the history-id.\n",
    "<br><br>More information including available galaxy-related environment variables can be found at https://github.com/bgruening/docker-jupyter-notebook. This notebook is running in a docker container based on the Docker Jupyter container described in that link.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling (Python) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/02\n",
      "importing Jupyter notebook from setPath.ipynb\n",
      "importing Jupyter notebook from /storage/galaxy/jobs_directory/003/3098/working/jupyter/Input/OpenDataPandaFramework13TeV.ipynb\n",
      "This library contains handy functions to ease the access and use of the 13TeV ATLAS OpenData release\n",
      "\n",
      "getBkgCategories()\n",
      "\t Dumps the name of the various background cataegories available \n",
      "\t as well as the number of samples contained in each category.\n",
      "\t Returns a vector with the name of the categories\n",
      "\n",
      "getSamplesInCategory(cat)\n",
      "\t Dumps the name of the samples contained in a given category (cat)\n",
      "\t Returns dictionary with keys being DSIDs and values physics process name from filename.\n",
      "\n",
      "getMCCategory()\n",
      "\t Returns dictionary with keys DSID and values MC category\n",
      "\n",
      "initialize(indir)\n",
      "\t Collects all the root files available in a certain directory (indir)\n",
      "\n",
      "\n",
      "\n",
      "Setting luminosity to 10064 pb^-1\n",
      "\n",
      "###############################\n",
      "#### Background categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "Diboson                      10\n",
      "Higgs                        20\n",
      "Wjets                        42\n",
      "Wjetsincl                     6\n",
      "Zjets                        42\n",
      "Zjetsincl                     3\n",
      "singleTop                     6\n",
      "topX                          3\n",
      "ttbar                         1\n",
      "###############################\n",
      "#### Signal categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "GG_ttn1                       4\n",
      "Gee                           5\n",
      "Gmumu                         5\n",
      "RS_G_ZZ                       5\n",
      "SUSYC1C1                     10\n",
      "SUSYC1N2                     18\n",
      "SUSYSlepSlep                 14\n",
      "TT_directTT                   4\n",
      "ZPrimeee                      4\n",
      "ZPrimemumu                    4\n",
      "ZPrimett                     12\n",
      "dmV_Zll                      10\n"
     ]
    }
   ],
   "source": [
    "import ROOT as R\n",
    "import import_ipynb\n",
    "import setPath\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from Input.OpenDataPandaFramework13TeV import *\n",
    "%jsroot on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the analaysis to run (*1largeRjet1lep*, *1lep1tau*, *3lep*, *exactly2lep*, *GamGam*, *2lep*, *4lep*)\n",
    "\n",
    "Set the directory where you have downloaded the ATLAS OpenData samples you want to run over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opendatadir = \"/storage/shared/data/fys5555/ATLAS_opendata/\"\n",
    "analysis = \"2lep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = R.TChain(\"mini\")\n",
    "data = R.TChain(\"mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of all the background samples, category and their IDs can be found in **Infofile.txt**. The cross-section, efficiencies etc. needed for scaling are stored in the **Files_<---->**. We read these files and add all the samples to the TChain. We also (for later convenience) make a vector containing the dataset IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "BACKGROUND SAMPLES\n",
      "####################################################################################################\n",
      "WARNING \t File for ggH125_tautaulh not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for VBFH125_tautaulh not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "####################################################################################################\n",
      "SIGNAL SAMPLES\n",
      "####################################################################################################\n",
      "WARNING \t File for ttH125_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for ggH125_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for VBFH125_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for WpH125J_Wincl_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for ZH125J_Zincl_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File data.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "###############################\n",
      "#### Background categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "Diboson                      10\n",
      "Higgs                        20\n",
      "Wjets                        42\n",
      "Wjetsincl                     6\n",
      "Zjets                        42\n",
      "Zjetsincl                     3\n",
      "singleTop                     6\n",
      "topX                          3\n",
      "ttbar                         1\n"
     ]
    }
   ],
   "source": [
    "mcfiles = initialize(opendatadir+\"/\"+analysis+\"/MC\")\n",
    "datafiles = initialize(opendatadir+\"/\"+analysis+\"/Data\")\n",
    "allfiles = z = {**mcfiles, **datafiles}\n",
    "Backgrounds = getBkgCategories() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCcat = {}\n",
    "for cat in allfiles:\n",
    "    for dsid in allfiles[cat][\"dsid\"]:\n",
    "        try:\n",
    "            MCcat[int(dsid)] = cat\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 118288518 entries for backgrounds\n"
     ]
    }
   ],
   "source": [
    "dataset_IDs = []\n",
    "background.Reset()\n",
    "for b in Backgrounds:\n",
    "    i = 0\n",
    "    if not b in mcfiles.keys(): continue\n",
    "    for mc in mcfiles[b][\"files\"]:\n",
    "        if not os.path.isfile(mc): continue\n",
    "        try:\n",
    "            dataset_IDs.append(int(mcfiles[b][\"dsid\"][i]))\n",
    "            background.Add(mc)\n",
    "        except:\n",
    "            print(\"Could not get DSID for %s. Skipping\"%mc)\n",
    "        i += 1\n",
    "nen = background.GetEntries()\n",
    "print(\"Added %i entries for backgrounds\"%(nen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 24411580 entries for data\n"
     ]
    }
   ],
   "source": [
    "data.Reset(); \n",
    "for d in datafiles[\"data\"][\"files\"]:  \n",
    "    if not os.path.isfile(d): continue\n",
    "    data.Add(d)\n",
    "nen = data.GetEntries()\n",
    "print(\"Added %i entries for data\"%(nen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Event selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning using (semi)-unsupervised learning, we need as unbiased data as possible. Thus, we need to construct a dataframe with all the neccesary features, which is a lot of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable or alias lumi\n"
     ]
    }
   ],
   "source": [
    "# Retrieve lumi from library\n",
    "%store -r lumi\n",
    "\n",
    "l1 = R.TLorentzVector()\n",
    "l2 = R.TLorentzVector()\n",
    "\n",
    "dilepton = R.TLorentzVector()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cell where the analysis is performed. Note that the cell needs to be run twice:\n",
    "\n",
    "1. with data = 0 to run over MC\n",
    "2. with data = 1 to run over data\n",
    "\n",
    "Note that the MC running takes ~5 minutes for 3lep analysis. Much(!!!) more time for e.g. 2lep analysis! Data running is relatively fast for 3lep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "\n",
    "isData = 0; \n",
    "\n",
    "if isData == 1: ds = data \n",
    "else: ds = background\n",
    "\n",
    "legal_flavor_tot = [13*2, 11*2]\n",
    "\n",
    "columns = {\"met\":[], \"XSection\":[], \n",
    "           \"tot_lep_invariant_mass\":[], \"lep0_pt\":[], \"lep1_pt\":[], \n",
    "           \"lep_z0_0\":[], \"lep_z0_1\":[], \"lep0_E\":[], \"lep1_E\":[],\n",
    "           \"lep_ptcone30_0\":[],\"lep_ptcone30_1\":[], \n",
    "           \"lep_etcone20_0\":[], \"lep_etcone20_1\":[],\n",
    "           \"lep_eta0\":[], \"lep_phi0\":[], \"lep_eta1\":[], \"lep_phi1\":[],\n",
    "           \"lep_trackd0pvunbiased_0\":[], \"lep_trackd0pvunbiased_1\":[],\n",
    "           \"lep_tracksigd0pvunbiased_0\":[], \"lep_tracksigd0pvunbiased_1\":[],\n",
    "           \"lep_type\":[], \"jet_n\":[], \"largeRjet_n\":[],\"tau_n\":[] , \"mcWeight\":[]\n",
    "          }\n",
    "\n",
    "lep0 = 0\n",
    "lep1 = 1\n",
    "\n",
    "i = 0   \n",
    "for event in ds:\n",
    "    if i == 100:\n",
    "        break\n",
    "    if i%10000 == 0 and i>0: \n",
    "        print(\"Total events {:.3f}%\".format(i/ds.GetEntries()*100))\n",
    "\n",
    "    i += 1 \n",
    "    # First event selection, require charge conservation, and lep flavor, and \n",
    "    # to only have the two largest leptons    \n",
    "    \n",
    "\n",
    "    ## Cut #1: Require (exactly) 2 leptons\n",
    "    if not ds.lep_n == 2: continue\n",
    "    ## Cut #2: Require opposite charge\n",
    "    if not ds.lep_charge[0] + ds.lep_charge[1] == 0 : continue\n",
    "    ## Cut #3: Require same flavour (2 electrons or 2 muons)\n",
    "    if not ds.lep_type[0] + ds.lep_type[1] in legal_flavor_tot: continue\n",
    "\n",
    "    \n",
    "    ## Require \"good leptons\": \n",
    "    \n",
    "    if ds.lep_pt[0]/1000.0 < 25: continue\n",
    "    if ds.lep_etcone20[0]/ds.lep_pt[0] > 0.30: continue\n",
    "    if ds.lep_ptcone30[0]/ds.lep_pt[0] > 0.30: continue\n",
    "    #if not (ds.lep_flag[0] & 512): continue\n",
    "        \n",
    "    if ds.lep_pt[1]/1000.0 < 25: continue\n",
    "    if ds.lep_etcone20[1]/ds.lep_pt[1] > 0.30: continue\n",
    "    if ds.lep_ptcone30[1]/ds.lep_pt[1] > 0.30: continue\n",
    "\n",
    "    \n",
    "    l1.SetPtEtaPhiE(ds.lep_pt[0]/1000., ds.lep_eta[0],\n",
    "                    ds.lep_phi[0], ds.lep_E[0]/1000.)\n",
    "    l2.SetPtEtaPhiE(ds.lep_pt[1]/1000., ds.lep_eta[1],\n",
    "                    ds.lep_phi[1], ds.lep_E[1]/1000.)\n",
    "    \n",
    "    dilepton = l1 + l2 \n",
    "    \n",
    "    dilep_inv_mass = dilepton.M()\n",
    "    \n",
    "    \n",
    "    ### General information \n",
    "    ### General information \n",
    "    columns[\"met\"].append(ds.met_et/1000.0)\n",
    "    columns[\"XSection\"].append(ds.XSection)  \n",
    "    columns[\"mcWeight\"].append(ds.mcWeight)\n",
    "    \n",
    "    ### Lep information\n",
    "    \n",
    "    columns[\"tot_lep_invariant_mass\"].append(dilep_inv_mass)\n",
    "    \n",
    "    columns[\"lep0_pt\"].append((ds.lep_pt[lep0])/1000)\n",
    "    columns[\"lep1_pt\"].append((ds.lep_pt[lep1])/1000)\n",
    "    \n",
    "    columns[\"lep0_E\"].append((ds.lep_E[lep0])/1000)\n",
    "    columns[\"lep1_E\"].append((ds.lep_E[lep1])/1000)\n",
    "    \n",
    "    columns[\"lep_ptcone30_0\"].append((ds.lep_ptcone30[lep0])/(ds.lep_pt[lep0]))\n",
    "    columns[\"lep_ptcone30_1\"].append((ds.lep_ptcone30[lep1])/(ds.lep_pt[lep1]))\n",
    "    \n",
    "    columns[\"lep_etcone20_0\"].append((ds.lep_etcone20[lep0])/(ds.lep_pt[lep0]))\n",
    "    columns[\"lep_etcone20_1\"].append((ds.lep_etcone20[lep1])/(ds.lep_pt[lep1]))\n",
    "    \n",
    "    columns[\"lep_eta0\"].append((ds.lep_eta[lep0]))\n",
    "    columns[\"lep_eta1\"].append((ds.lep_eta[lep1]))\n",
    "    \n",
    "    columns[\"lep_phi0\"].append((ds.lep_phi[lep0]))\n",
    "    columns[\"lep_phi1\"].append((ds.lep_phi[lep1]))\n",
    "    \n",
    "    columns[\"lep_z0_0\"].append(ds.lep_z0[lep0])\n",
    "    columns[\"lep_z0_1\"].append(ds.lep_z0[lep1])\n",
    "    \n",
    "    columns[\"lep_trackd0pvunbiased_0\"].append(ds.lep_trackd0pvunbiased[lep0])\n",
    "    columns[\"lep_trackd0pvunbiased_1\"].append(ds.lep_trackd0pvunbiased[lep1])\n",
    "    \n",
    "    columns[\"lep_tracksigd0pvunbiased_0\"].append(ds.lep_tracksigd0pvunbiased[lep0])\n",
    "    columns[\"lep_tracksigd0pvunbiased_1\"].append(ds.lep_tracksigd0pvunbiased[lep1])\n",
    "\n",
    "    columns[\"lep_type\"].append(ds.lep_type[lep0])\n",
    "    columns[\"jet_n\"].append(ds.jet_n)\n",
    "    columns[\"largeRjet_n\"].append(ds.largeRjet_n)\n",
    "    columns[\"tau_n\"].append(ds.tau_n)\n",
    "\n",
    "print(\"hello\")\n",
    "df = pd.DataFrame(columns)\n",
    "    \n",
    "\n",
    "print(\"Done!\")\n",
    "if isData == 0:\n",
    "    df.to_hdf('mcdata.h5', key='df', mode='w')\n",
    "    print(\"Remebered to run over data? No? Set data = 1 at the top and run again\")\n",
    "else:\n",
    "    df.to_hdf('testdata.h5', key='df', mode='w')\n",
    "    print(\"Remebered to run over MC? No? Set data = 0 at the top and run again\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File mcdata.h5 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2920/1962775365.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mcdata.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/software/easybuild/software/DataAnalysis/1.0.0/lib/python3.7/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File {path_or_buf} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File mcdata.h5 does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_hdf(\"mcdata.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
