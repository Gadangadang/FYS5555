{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### When running this notebook via the Galaxy portal\n",
    "You can access your data via the dataset number. Using a Python kernel, you can access dataset number 42 with ``handle = open(get(42), 'r')``.\n",
    "To save data, write your data to a file, and then call ``put('filename.txt')``. The dataset will then be available in your galaxy history.\n",
    "<br><br>Note that if you are putting/getting to/from a different history than your default history, you must also provide the history-id.\n",
    "<br><br>More information including available galaxy-related environment variables can be found at https://github.com/bgruening/docker-jupyter-notebook. This notebook is running in a docker container based on the Docker Jupyter container described in that link.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Handling (Python) \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import ROOT as R\n",
    "import import_ipynb\n",
    "import setPath\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from Input.OpenDataPandaFramework13TeV import *\n",
    "%jsroot on"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Welcome to JupyROOT 6.24/02\n",
      "importing Jupyter notebook from setPath.ipynb\n",
      "importing Jupyter notebook from /storage/galaxy/jobs_directory/003/3068/working/jupyter/Input/OpenDataPandaFramework13TeV.ipynb\n",
      "This library contains handy functions to ease the access and use of the 13TeV ATLAS OpenData release\n",
      "\n",
      "getBkgCategories()\n",
      "\t Dumps the name of the various background cataegories available \n",
      "\t as well as the number of samples contained in each category.\n",
      "\t Returns a vector with the name of the categories\n",
      "\n",
      "getSamplesInCategory(cat)\n",
      "\t Dumps the name of the samples contained in a given category (cat)\n",
      "\t Returns dictionary with keys being DSIDs and values physics process name from filename.\n",
      "\n",
      "getMCCategory()\n",
      "\t Returns dictionary with keys DSID and values MC category\n",
      "\n",
      "initialize(indir)\n",
      "\t Collects all the root files available in a certain directory (indir)\n",
      "\n",
      "\n",
      "\n",
      "Setting luminosity to 10064 pb^-1\n",
      "\n",
      "###############################\n",
      "#### Background categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "Diboson                      10\n",
      "Higgs                        20\n",
      "Wjets                        42\n",
      "Wjetsincl                     6\n",
      "Zjets                        42\n",
      "Zjetsincl                     3\n",
      "singleTop                     6\n",
      "topX                          3\n",
      "ttbar                         1\n",
      "###############################\n",
      "#### Signal categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "GG_ttn1                       4\n",
      "Gee                           5\n",
      "Gmumu                         5\n",
      "RS_G_ZZ                       5\n",
      "SUSYC1C1                     10\n",
      "SUSYC1N2                     18\n",
      "SUSYSlepSlep                 14\n",
      "TT_directTT                   4\n",
      "ZPrimeee                      4\n",
      "ZPrimemumu                    4\n",
      "ZPrimett                     12\n",
      "dmV_Zll                      10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Reading the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set the analaysis to run (*1largeRjet1lep*, *1lep1tau*, *3lep*, *exactly2lep*, *GamGam*, *2lep*, *4lep*)\n",
    "\n",
    "Set the directory where you have downloaded the ATLAS OpenData samples you want to run over"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "opendatadir = \"/storage/shared/data/fys5555/ATLAS_opendata/\"\n",
    "analysis = \"2lep\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "background = R.TChain(\"mini\")\n",
    "data = R.TChain(\"mini\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A list of all the background samples, category and their IDs can be found in **Infofile.txt**. The cross-section, efficiencies etc. needed for scaling are stored in the **Files_<---->**. We read these files and add all the samples to the TChain. We also (for later convenience) make a vector containing the dataset IDs. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "mcfiles = initialize(opendatadir+\"/\"+analysis+\"/MC\")\n",
    "datafiles = initialize(opendatadir+\"/\"+analysis+\"/Data\")\n",
    "allfiles = z = {**mcfiles, **datafiles}\n",
    "Backgrounds = getBkgCategories() "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING \t File topX.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Zjetsincl.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Diboson.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Gmumu.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File ZPrimeee.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File dmV_Zll.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File RS_G_ZZ.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Higgs.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File ZPrimemumu.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File ZPrimett.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Wjetsincl.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Gee.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Zjets.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File Wjets.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File SUSYC1C1.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File GG_ttn1.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File TT_directTT.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File SUSYC1N2.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File SUSYSlepSlep.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File ttbar.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "WARNING \t File singleTop.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "####################################################################################################\n",
      "BACKGROUND SAMPLES\n",
      "####################################################################################################\n",
      "WARNING \t File for ggH125_tautaulh not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for VBFH125_tautaulh not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "####################################################################################################\n",
      "SIGNAL SAMPLES\n",
      "####################################################################################################\n",
      "WARNING \t File for ttH125_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for ggH125_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for VBFH125_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for WpH125J_Wincl_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File for ZH125J_Zincl_gamgam not found in /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC\n",
      "WARNING \t File data.root not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\n",
      "###############################\n",
      "#### Background categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "Diboson                      10\n",
      "Higgs                        20\n",
      "Wjets                        42\n",
      "Wjetsincl                     6\n",
      "Zjets                        42\n",
      "Zjetsincl                     3\n",
      "singleTop                     6\n",
      "topX                          3\n",
      "ttbar                         1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "MCcat = {}\n",
    "for cat in allfiles:\n",
    "    for dsid in allfiles[cat][\"dsid\"]:\n",
    "        try:\n",
    "            MCcat[int(dsid)] = cat\n",
    "        except:\n",
    "            continue"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "dataset_IDs = []\n",
    "background.Reset()\n",
    "for b in Backgrounds:\n",
    "    i = 0\n",
    "    if not b in mcfiles.keys(): continue\n",
    "    for mc in mcfiles[b][\"files\"]:\n",
    "        if not os.path.isfile(mc): continue\n",
    "        try:\n",
    "            dataset_IDs.append(int(mcfiles[b][\"dsid\"][i]))\n",
    "            background.Add(mc)\n",
    "        except:\n",
    "            print(\"Could not get DSID for %s. Skipping\"%mc)\n",
    "        i += 1\n",
    "nen = background.GetEntries()\n",
    "print(\"Added %i entries for backgrounds\"%(nen))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Could not get DSID for /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC/Diboson.root. Skipping\n",
      "Could not get DSID for /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC/Higgs.root. Skipping\n",
      "Could not get DSID for /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC/Wjets.root. Skipping\n",
      "Could not get DSID for /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC/Wjetsincl.root. Skipping\n",
      "Could not get DSID for /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC/Zjets.root. Skipping\n",
      "Could not get DSID for /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC/Zjetsincl.root. Skipping\n",
      "Could not get DSID for /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC/singleTop.root. Skipping\n",
      "Could not get DSID for /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC/topX.root. Skipping\n",
      "Could not get DSID for /storage/shared/data/fys5555/ATLAS_opendata//2lep/MC/ttbar.root. Skipping\n",
      "Added 118288518 entries for backgrounds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "data.Reset(); \n",
    "for d in datafiles[\"data\"][\"files\"]:  \n",
    "    if not os.path.isfile(d): continue\n",
    "    data.Add(d)\n",
    "nen = data.GetEntries()\n",
    "print(\"Added %i entries for data\"%(nen))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Added 24411580 entries for data\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Event selection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For machine learning using (semi)-unsupervised learning, we need as unbiased data as possible. Thus, we need to construct a dataframe with all the neccesary features, which is a lot of data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Retrieve lumi from library\n",
    "%store -r lumi\n",
    "\n",
    "l1 = R.TLorentzVector()\n",
    "l2 = R.TLorentzVector()\n",
    "\n",
    "dilepton = R.TLorentzVector()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "no stored variable or alias lumi\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the cell where the analysis is performed. Note that the cell needs to be run twice:\n",
    "\n",
    "1. with data = 0 to run over MC\n",
    "2. with data = 1 to run over data\n",
    "\n",
    "Note that the MC running takes ~5 minutes for 3lep analysis. Much(!!!) more time for e.g. 2lep analysis! Data running is relatively fast for 3lep. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "%%time\n",
    "import time\n",
    "import numpy as np\n",
    "isData = 0; \n",
    "\n",
    "if isData == 1: ds = data \n",
    "else: ds = background\n",
    "\n",
    "legal_flavor_tot = [13*4, 11*4]\n",
    "    \n",
    "columns = {\"met\":[], \"XSection\":[], \n",
    "           \"lep_n \":[],\"tot_lep_invariant_mass\":[], \"mean_lep_pt\":[], \"mean_lep_E\":[], \"mean_lep_ptcone30\":[], \"mean_lep_etcone20\":[], \"mean_lep_eta\":[], \"mean_lep_phi\":[],\n",
    "           \"jet_n\":[], \"mean_jet_pt\":[], \"mean_jet_E\":[], \"mean_jet_eta\":[], \"mean_jet_phi\":[],\n",
    "           \"photon_n\":[], \"mean_photon_pt\":[], \"mean_photon_E\":[], \"mean_photon_ptcone30\":[], \"mean_photon_etcone20\":[],\"mean_photon_eta\":[], \"mean_photon_phi\":[],\n",
    "           \"largeRjet_n\":[], \"tot_largeRjet_m\":[],\"mean_largeRjet_pt\":[], \"mean_largeRjet_E\":[], \"mean_largeRjet_eta\":[], \"mean_largeRjet_phi\":[],\n",
    "           \"tau_n\":[], \"mean_tau_pt\":[], \"mean_tau_E\":[], \"mean_tau_eta\":[], \"mean_tau_phi\":[],\n",
    "           \"mean_lep_pt_syst\":[], \"met_et_syst \":[], \n",
    "           \"mean_jet_pt_syst\":[], \"mean_photon_pt_syst\":[], \n",
    "           \"mean_largeRjet_pt_syst\":[], \"mean_tau_pt_syst\":[]\n",
    "          }\n",
    "\n",
    "i = 0   \n",
    "for event in ds: \n",
    "    \n",
    "    if i%100000 == 0 and i>0: \n",
    "        print(\"Total events %i/%i\"%(i,ds.GetEntries()))\n",
    "    i += 1 \n",
    "   \n",
    "    # First event selection, require charge conservation, and lep flavor, and \n",
    "    # to only have the two largest leptons    \n",
    "    \n",
    "    ## Cut #1: Require 2 or more leptons, but must find the indices of the two we pick\n",
    "    if not ds.lep_n >= 2: continue\n",
    "    lep_pt = np.zeros(ds.lep_n)\n",
    "    lep_type = np.zeros(ds.lep_n)\n",
    "    for i in range(ds.lep_n):\n",
    "        lep_pt[i] = ds.lep_pt[i]\n",
    "        lep_type = ds.lep_type[i]\n",
    "        \n",
    "    elec_index = np.where(lep_type == 11)[0]\n",
    "    muon_index = np.where(lep_type == 13)[0]\n",
    "    \n",
    "    if (len(muon_index) < len(elec_index)) and (len(elec_index) == 2):\n",
    "        lep_index = elec_index\n",
    "    elif (len(elec_index) < len(muon_index)) and (len(muon_index) == 2): \n",
    "        lep_index = muon_index\n",
    "    else:  \n",
    "        max_index = np.where(lep_pt == np.max(lep_pt))[0]\n",
    "        lep_typ = lep_type[max_index]\n",
    "        lep_ind = np.where(lep_type == lep_typ)[0]\n",
    "        \n",
    "    lep0 = lep_index[0]\n",
    "    lep1 = lep_index[1]\n",
    "    \n",
    "    \n",
    "    ## Cut #2: Require opposite charge\n",
    "    if not ds.lep_charge[lep0] + ds.lep_charge[lep1] == 0 : continue\n",
    "    \n",
    "\n",
    "    \n",
    "    ## Require \"good leptons\": \n",
    "    \n",
    "    if ds.lep_pt[lep0]/1000.0 < 25: continue\n",
    "    if ds.lep_etcone20[lep0]/ds.lep_pt[lep0] > 0.15:\n",
    "        continue\n",
    "    if ds.lep_ptcone30[lep0]/ds.lep_pt[lep0] > 0.15:\n",
    "        continue\n",
    "    #if not (ds.lep_flag[0] & 512): continue\n",
    "        \n",
    "    if ds.lep_pt[lep1]/1000.0 < 25:\n",
    "        continue\n",
    "    if ds.lep_etcone20[lep1]/ds.lep_pt[lep1] > 0.15:\n",
    "        continue\n",
    "    if ds.lep_ptcone30[lep1]/ds.lep_pt[lep1] > 0.15:\n",
    "        continue\n",
    "    #if not (ds.lep_flag[1] & 512): continue\n",
    "    \n",
    "    l1.SetPtEtaPhiE(ds.lep_pt[lep0]/1000., ds.lep_eta[lep0],\n",
    "                    ds.lep_phi[lep0], ds.lep_E[lep0]/1000.)\n",
    "    l2.SetPtEtaPhiE(ds.lep_pt[lep1]/1000., ds.lep_eta[lep1],\n",
    "                    ds.lep_phi[lep1], ds.lep_E[lep1]/1000.)\n",
    "    \n",
    "    dilepton = l1 + l2 \n",
    "    \n",
    "    dilep_inv_mass = dilepton.M()\n",
    "    ## Event selection:\n",
    "    \n",
    "    ### General information \n",
    "    columns[\"met\"].append(ds.met_et/1000.0)\n",
    "    columns[\"XSection\"].append(ds.XSection)  \n",
    "    \n",
    "    ### Lep information\n",
    "    columns[\"lep_n\"].append(ds.lep_n)\n",
    "    columns[\"tot_lep_invariant_mass\"].append(dilep_inv_mass)\n",
    "    columns[\"mean_lep_pt\"].append(np.mean(ds.lep_pt))\n",
    "    columns[\"mean_lep_E\"].append(np.mean(ds.lep_E))\n",
    "    columns[\"mean_lep_ptcone30\"].append(np.mean(ds.lep_ptcone30))\n",
    "    columns[\"mean_lep_etcone20\"].append(np.mean(ds.lep_etcone20))\n",
    "    columns[\"mean_lep_eta\"].append(np.mean(ds.lep_eta))\n",
    "    columns[\"mean_lep_phi\"].append(np.mean(ds.lep_phi))\n",
    "    \n",
    "    ### Jet information\n",
    "    columns[\"jet_n\"].append(ds.jet_n)\n",
    "    columns[\"mean_jet_pt\"].append(np.mean(ds.jet_pt))\n",
    "    columns[\"mean_jet_E\"].append(np.mean(ds.jet_E))\n",
    "    columns[\"mean_jet_eta\"].append(np.mean(ds.jet_eta))\n",
    "    columns[\"mean_jet_phi\"].append(np.mean(ds.jet_phi))\n",
    "    \n",
    "    ### Photon information\n",
    "    columns[\"photon_n\"].append(ds.photon_n)\n",
    "    columns[\"mean_photon_pt\"].append(np.mean(ds.photon_pt))\n",
    "    columns[\"mean_photon_E\"].append(np.mean(ds.photon_E))\n",
    "    columns[\"mean_photon_ptcone30\"].append(np.mean(ds.photon_ptcone30))\n",
    "    columns[\"mean_photon_etcone20\"].append(np.mean(ds.photon_etcone20))\n",
    "    columns[\"mean_photon_eta\"].append(np.mean(ds.photon_eta))\n",
    "    columns[\"mean_photon_phi\"].append(np.mean(ds.photon_phi))\n",
    "    \n",
    "    ### LargeRjet information\n",
    "    columns[\"largeRjet_n\"].append(ds.largeRjet_n)\n",
    "    columns[\"tot_largeRjet_m\"].append(ds.largeRjet_m)\n",
    "    columns[\"mean_largeRjet_pt\"].append(np.mean(ds.largeRjet_pt))\n",
    "    columns[\"mean_largeRjet_E\"].append(np.mean(ds.largeRjet_E))\n",
    "    columns[\"mean_largeRjet_eta\"].append(np.mean(ds.largeRjet_eta))\n",
    "    columns[\"mean_largeRjet_phi\"].append(np.mean(ds.largeRjet_phi))\n",
    "    \n",
    "    \n",
    "    ### Tau information \n",
    "    columns[\"tau_n\"].append(ds.tau_n)\n",
    "    columns[\"mean_tau_pt\"].append(np.mean(ds.tau_pt))\n",
    "    columns[\"mean_tau_E\"].append(np.mean(ds.tau_E))\n",
    "    columns[\"mean_tau_eta\"].append(np.mean(ds.tau_eta))\n",
    "    columns[\"mean_tau_phi\"].append(np.mean(ds.tau_phi))\n",
    "    \n",
    "    \n",
    "    ### Systematic uncertainty\n",
    "    columns[\"mean_lep_pt_syst\"].append(ds.lep_pt_syst)\n",
    "    columns[\"met_et_syst \"].append(ds.met_et_syst)\n",
    "    columns[\"mean_jet_pt_syst\"].append(ds.jet_pt_syst)\n",
    "    columns[\"mean_photon_pt_syst\"].append(ds.photon_pt_syst)\n",
    "    columns[\"mean_largeRjet_pt_syst\"].append(ds.largeRjet_pt_syst)\n",
    "    columns[\"mean_tau_pt_syst\"].append(ds.tau_pt_syst)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "df = pd.DataFrame(data=columns)\n",
    "        \n",
    "print(\"Done!\")\n",
    "if isData == 0:\n",
    "    print(\"Remebered to run over data? No? Set data = 1 at the top and run again\")\n",
    "else:\n",
    "    print(\"Remebered to run over MC? No? Set data = 0 at the top and run again\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total events 100000/1601489\n",
      "Done!\n",
      "Remebered to run over data? No? Set data = 1 at the top and run again\n",
      "CPU times: user 20.1 s, sys: 198 ms, total: 20.3 s\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.to_hdf(\"datatest.csv\",\"mini\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}