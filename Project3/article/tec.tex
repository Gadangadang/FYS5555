% For instructions, 
\documentclass[ reprint, amsmath,amssymb, aps, nofootinbib]{revtex4-2}
\usepackage[T1]{fontenc} %for å bruke æøå
\usepackage[utf8]{inputenc}
\usepackage{graphicx} %for å inkludere grafikk
\usepackage{verbatim} %for å inkludere filer med tegn LaTeX ikke liker
\usepackage{mathpazo}
\usepackage{amsfonts,amsthm}
\usepackage{amsmath}
\usepackage{url}
\usepackage{float}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage[style=numeric]{biblatex}
\usepackage[noend]{algpseudocode}
%\usepackage[style=numeric]{biblatex}
%\usepackage{comment}
\addbibresource{Reference.bib}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{caption}
\usepackage[flushleft]{threeparttable}
\usepackage{array,booktabs,makecell}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{subcaption}
%\usepackage{algorithm2e}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{siunitx}
\usepackage{mathtools}
\usepackage{lipsum}
\usepackage{abstract}
%\usepackage{tocloft}
%\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}} 
\usepackage{hyperref}
\usepackage{tikz} 
\usetikzlibrary{shapes,arrows,positioning,automata,backgrounds,calc,er,patterns}
\usepackage{tikz-feynman}
\tikzfeynmanset{compat=1.0.0}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\usepackage{float}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\urlstyle{same}


\renewcommand{\vec}[1]{\mathbf{#1}} % \vec gives bold text instead of arrow
\DeclareMathOperator*{\E}{\mathbb{E}}



\begin{document}
\begin{titlepage}
\centering
\vspace*{2cm}


%{\Large Higgs boson classification using multiple classification methods}
{\Large\bfseries Deployment of unsupervised learning in a search for new physics with ATLAS Open Data}

\vspace{1cm}
{\Large Testing of auto encoder for semi unsupervised learning}

\vspace{1cm}
{\large Sakarias Frette}

\vspace{1cm}
{\bfseries Spring 2022}
\vspace{0.5cm}

\begin{center}

\begin{abstract}
Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Ut purus elit, vestibulum ut, placerat ac,
adipiscing vitae, felis. Curabitur dictum gravida mauris. Nam arcu libero, nonummy eget, consectetuer
id, vulputate a, magna. Donec vehicula augue eu neque. Pellentesque habitant morbi tristique senectus
et netus et malesuada fames ac turpis egestas. Mauris ut leo. Cras viverra metus rhoncus sem. Nulla
et lectus vestibulum urna fringilla ultrices. Phasellus eu tellus sit amet tortor gravida placerat. Integer
sapien est, iaculis in, pretium quis, viverra ac, nunc. Praesent eget sem vel leo ultrices bibendum. Aenean
faucibus. Morbi dolor nulla, malesuada eu, pulvinar at, mollis ac, nulla. Curabitur auctor semper nulla.
Donec varius orci eget risus. Duis nibh mi, congue eu, accumsan eleifend, sagittis quis, diam. Duis eget
orci sit amet orci dignissim rutrum.
\end{abstract}
\end{center}
\vspace{2.5cm}

{\itshape University of Oslo}
\end{titlepage}
\clearpage
\newpage
\mbox{~}
\clearpage
\newpage

\tableofcontents




\section{Introduction}
The standard model is arguably the best model ever created by man, showing remarkable accuracy comparing with experiments. Its crown jewel was discovered in 2012 by both ATLAS\footnote{Paper from ATLAS collaboration can be found \href{https://arxiv.org/abs/1207.7214}{here}.} and CMS\footnote{Paper from CMS collaboration can be found \href{https://arxiv.org/abs/1207.7235}{here}.} at CERN. There has however not been any new particle discoveries since the early 70's, and this is somewhat alarming. The standard model has several issues, some of them includes lack of an explanation of dark energy, cosmic inflation, the hierarchy problem and not including gravity. \par 
This leads scientists to look beyond the standard model, and there are several models proposed, such as dark matter candidates, extra vector bosons and super symmetry. Attempts are made by multiple collaborations to discover evidence for these models, and there are several obstacles to overcome, such as method of analysis and sufficient amount of data. With the impressive progress of machine learning software such as Tensorflow\cite{tensorflow2015-whitepaper}, machine learning methods have become more and more popular as possible methods  to use for searching for new physics. \par 
In the last decade machine learning has excelled from being tedious and hard to program to become available to almost everyone. It's scale-ability and ability to discover hidden structure in large datasets makes it an intriguing candidate to use on data from the Large Hadron Collider. One possible candidate is semi unsupervised learning, which is done by an auto encoder. The idea is to train a model-independent algorithm on datasets containing only known physics, with the hope that what ever new physics that is in the data is detected, without knowing what it is.  This machine learning model is the method of choice for this report. \par
\\
This report is structured in a theory, implementation, results and discussion, and conclusion section. In the theory section the necessary theory is introduced, with respect to both data, method of analysis and choice of evaluation. In the implementation section I discuss data handling , with respect to cuts, event selection, scaling and feature choices, choice of hyper parameters for hyper parameter search and the software and api's used for training and tuning. In the results and discussion section I display the results and discuss them as as well as distinct observations. In the conclusion I summarize my findings. 


\section{Theory}
Some of theory sections about anomaly detection and machine learning algorithms are based on previous work done in other courses, such as \cite{FYSSTK} and \cite{ML_PROJ}.

\subsection{Anomaly detection}

Anomaly detection is a tool with a wide range of uses, from time series data, fraud detection or anomalous sensor data. Its main purpose is to detect data which does not conform to some predetermined standard for normal behavior. The predetermined standard varies from situation to situation, and can be set by the context it self, and what is expected as an anomaly. Anomalies are typically classified in three categories\cite{anom_detec}:
\begin{enumerate}
    \item Point anomalies
    \item Contextual anomalies
    \item Collective anomalies
\end{enumerate}


For the purpose of this report we will mostly consider collective anomalies, as anomalies in high energy physics has to be collective to claim anything, as noise and other factors can and often do create point or event collective anomalies. Through triggers\footnote{ATLAS experiment \href{https://atlas.cern/Discover/Detector/Trigger-DAQ}{website}.}, one hopes that most of the noise and other factors are accounted for, but it is not a perfect system.\par 

In high energy physics we can, using machine learning, separate anomaly detection into two categories, supervised and unsupervised searches. . Figure \ref{fig:corr_class}, \ref{fig:wrong_class} and \ref{fig:unsup_class} are not based on actual predictions and are meant to only illustrate.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/theory/correct_class.pdf}
    \caption{Example of supervised classification of anomaly where the target model is correct. Here the machine learning model manages to correctly identify the anomalies. }
    \label{fig:corr_class}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/theory/wrong_class.pdf}
    \caption{Example of supervised classification of anomaly where the target model is wrong. Here the machine learning model manages to wrongly identify the anomalies. }
    \label{fig:wrong_class}
\end{figure}

In figure \ref{fig:corr_class} we see an example of supervised classification. Because the model trained to recognize certain anomalies that are in the data set in manages very well to classify them. In figure \ref{fig:wrong_class} we do however see that the same model does not find any anomalies, as they are completely different from the ones it trained on. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/theory/unsuper.pdf}
    \caption{Example of unsupervised classification of anomaly with no target model. Here the machine learning model manages to with good precision identify the anomalies. }
    \label{fig:unsup_class}
\end{figure}

In figure \ref{fig:unsup_class} we have an example of unsupervised learning. Here we do not tell what is anomalies and what is expected behavior. Note that the unsupervised classifier does not classify correctly every anomaly, and the grade of which it does vary from model to model, and from dataset to dataset. \par \par 

There are several ways to declare events as anomalies and not expected events. One method used in time series data is to have a running average, and then if some values deviate with with x standard deviations, you tag that event. But because we are only interested in collective anomalies and not point anomalies, we need to use distributions to evaluate the output of the machine learning algorithm. A perfect classifier would for instance have two completely separate distributions, where as a poor classifier would have two distributions with overlap. An additional metric to use is the so called ROC curve, which checks the output of the classifier and finds how often the classifier predicts a true and a false positive. This can only be done with labeled data, thus is not itself a metric of whether or not the algorithm detected new physics, but rather how well it can distinguish known anomalies.

\subsection{Beyond standard model physics}



In the context of high energy physics, collective anomalies would be new physics we both do and do not have a model for.  Such anomalies could be super symmetric particles\cite{JMLR:v18:16-558}, new vector bosons or dark matter particles, as well as many other candidates. These types of searches require enormous amounts of data, and both supervised and unsupervised methods are tried continuously to see if there are "hidden" relations in the data that can separate the known standard model from the unknown new physics. \par 

The methods have their strengths and weaknesses, and should be deployed with this in mind. A supervised model will always be better to classify and separate distributions of background\footnote{Standard model processes created from Monte Carlo simulation will be denoted as background mc. These simulations are carefully compared and corrected against data from detectors that we with high certainty claim have only standard model processes.} and signal\footnote{New physics models are created using Monte Carlo simulation, and are based around theoretically allowed properties. They have not been discovered and cannot be considered "real" new physics until they do. In this project it is denoted as signal mc.} events, and if the signal actually exists in the data from the detector, it will give better accuracy than any (semi-) unsupervised methods. \par 
In the event that some signal actually exist in the data from the detector that we do not have a model for, a supervised model will not be able to recognize it purely as a signal, whilst the unsupervised might claim that there is something other than sm background. The certainty of the results will vary with different models, but in the case of an auto encoder, this is due to the fact that it will only recognize background events that is has trained on, and not any new physics. The drawback with using unsupervised methods in high energy physics, or other fields with multiple anomaly candidates, is that you cannot know what the signal is, only that is it not background. \par 



\subsection{Stacked auto encoder}
One method to attack the anomaly detection problem is the so called (stacked) auto encoder. The idea is based on reconstruction, and has been implemented for denoising of images, image compression, and anomaly detection. An auto encoder is a subgroup of feed forward neural networks\cite{FYSSTK}, and the goal is to compress the information into fewer variables, called the latent space, which then can explain much of the data through decoding that information. This allows the algorithm to learn the most important components of the data. An illustrative image of an auto encoder is shown below. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/theory/autoencoder-architecture.png}
    \caption{The architecture of an autoencoder with image reconstruction as the example use. \href{https://lilianweng.github.io/posts/2018-08-12-vae/}{Source}, accessed 28.04.22}
    \label{fig:auto_en_archi}
\end{figure}

We see here in figure \ref{fig:auto_en_archi} that the input data is deconstructed through an encoder to a lower dimensional space called the latent space, depicted here as \textbf{z}, and then reconstructed the data with the decoder. It is important to note that the number of layers, nodes per layer and activation function per layer for the encoder does not need to match the structure of the decoder. The only requirement is that the input and output layer has the same shape. The end result is the reconstructed data \textbf{x'}. The training of the model is parameterized in the following way. We define the decoder as 

\begin{equation*}
    \textbf{z} = g_{\xi}(\bf{x}),
\end{equation*}
and the reconstructed information as 
\begin{equation*}
    \textbf{x'} = f_{\theta}(g_{\xi}(\bf{x})),
\end{equation*}

where the parameters $(\xi, \theta)$ are tuned to reconstruct the data as close to the original data as possible. The model then adjusts following the simple mean squared error of the reconstruction and the actual data, given below

\begin{equation*}
    L_{AE}(\xi, \phi)= \frac{1}{N} \sum_{i=0}^{N-1}\bigg( \textbf{x}^i - f_{\theta}(g_{\xi}(\textbf{x}^i))\bigg)^2
\end{equation*}



\section{Implementation}

\subsection{Handling of data}

In appendix \ref{appendix:features} the features used in the data set are listed, with their respective datatype and description. The datasets are separated into two categories, background and signal mc and actual data from a detector. The totality of the data used in this report is given as ATLAS Open Data\footnote{Link to ATLAS Open Data \href{https://atlas.cern/resources/opendata}{here}.}. This is a dataset that has been analyzed by ATLAS for new physics, and have thus been released to the public. 

\subsubsection{Cuts, pre-selection and scaling}
To preserve as much information and add as little bias as possible very few cuts where made. The entire data gathering process is biased from the moment the data is collected at the collider, but even then it is good to try to minimize the bias. \par

The first cut I impose is to require "good leptons". This is done by requiring lep\_etcone$20/$lep\_pt $< 0.15$ and lep\_ptcone$30/$lep\_pt $< 0.15$. We then require only two leptons per event. The event selection was done using RDataframe\cite{rene_al._2019}, to speed up the selection\footnote{This code was written by Eirik Gramstad, and can be found \href{https://github.uio.no/zpath/software/blob/eirik-dev/Notebooks/ATLASOpenData/13TeV/RDataFrameToDF.ipynb}{here}.}\par 

I tested two methods of scaling in this report, Min Max Scaling\footnote{More on Min max scaling \href{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html}{here}.} and Standard scaling\footnote{More on Standard scaling \href{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html}{here}.}\cite{scikit-learn}.

\subsubsection{Choice of features}
The choice of features is a mixture of specification and broadness. In this context specification is defined as information that specify for a given part of the physical system, in this case the leptons. Thus, most of the features are directly related to the leptons. Broadness is in this context defined as information about the rest part of the final state, such as jets, photons or tau leptons. Ideally one would pick as much information as possible for each event, but there is a trade-off between amount of information and size of data, and as consequence, execution time. \par 
The features were picked such that there are no missing values, only 0 or larger than 0. For regular machine learning problems, missing values usually gets replaced by the mean value of the feature. This is however a problem when analysing physics, as such a choice could violate the laws of physics. To avoid this, all features are designed to either have 0 if missing, or sum up the contributions of both missing and present values, given that they represent the same type of information. \par 
An example of this is if we have two jets in one event and three in another, using the transverse momentum of the jets as features. We cannot in the first event input the mean values of all third jet-transverse momentum in the entire dataset, but we could put 0, or sum all the transverse momentum for the jets in the given event into one feature. Another possibility is to only give the count of jets in the event, and there are even more ways to solve this issue. 

\subsubsection{Signal samples}
To test the algorithm I use the following signal sample categories. There are five supersymmetric sample categories: SUSYSlepSlep, SUSYC1C1, GG\_ttn1, TT\_directTT and SUSYC1N2\cite{DeSanto:2644352}, three Z' boson sample categories: Zprime\_ee, Zprime\_mumu and Zprime\_tt\cite{https://doi.org/10.48550/arxiv.1308.5874}, three Randall-Sundrum graviton sample categories: RS\_G\_ZZ, G\_ee and G\_mumu\cite{Randall_1999}, and one dark matter mediator sample category: dmV\_Zll. 

 

\subsection{Tuning and training}
For the auto encoder to accurately reconstruct the standard model, the model needs to train on the background mc. However, neural networks are highly susceptible to hyperparameters, which needs tuning. In this project I used Keras-tuner\cite{omalley2019kerastuner} to tune the hyperparameters. The choice of tuner for this report was Hyperband\cite{JMLR:v18:16-558}. It is not well understood how the hyperparameters interact with each other to affect the result, thus there are really no guidelines for approaching this problem. A result of this might be that the choice of hyperparameters seem completely random or strange. The hyperparameters used in the network are the learning rate, the alpha parameter for the LeakyReLU\footnote{In Tensorflow LeakyReLU is implemented as a layer and not an activation function, but the layer can be passed as an activation argument, which leads to creating \href{https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_custom_objects}{custom objects } in the Keras backend when tuning with Keras-Tuner. More on the LeakyReLU api can be found \href{https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU}{here}.} activation function, the activation function for each layer, the amount of nodes per layer and regularization for layer's kernel and output. The regularization is added to prevent the algorithm from overfitting to much towards the background mc. The activation functions used in the hyperparameter search where
\begin{itemize}
    \item ReLU
    \item LeakyReLU
    \item Tanh
    \item Linear
\end{itemize}
whilst the different learning rates that was tested was $[9\cdot 10^{-2}, 9.5\cdot 10^{-2}, 10^{-3}, 1.5\cdot 10^{-3}]$, and the kernel and output regularization constant values were $[0.5, 0.1, 0.05, 0.01]$. The optimizer used for all the models is the ADAM\cite{https://doi.org/10.48550/arxiv.1412.6980} optimizer. \par \par 
The background mc where split into a 80-20 split, where 80\% were used for training and tuning, and 20\% were mixed with signal mc for testing. The 80\% training set what then again split with a 80-20 split for the hyperparameter search, called grid\_train and grid\_test. Due to the large amount of events in the background mc, I had to pick out a sample that had enough events to be a fair representation of the entire training set. Thus 10 million events were randomly sampled from grid\_train for hyperparameter training and 2 million events sampled from grid\_test for hyperparameter validation. \par 
The auto encoder was written the Tensorflow api\cite{tensorflow2015-whitepaper}\cite{chollet2015keras}, using a functional structure\footnote{Functional structure uses function call for layers, i.e for layers a,b, then b(a) will connect the two layers, and equals a sequential link a $\to$ b. This allows for more flexible structures. More on the functional api can be found \href{https://www.tensorflow.org/guide/keras/functional}{here}.}. In practise, this model could just as well have been written as a Sequential model\footnote{Sequential structure adds layers in sequence, i.e for layers a, b, c we have that a $\to$ b $\to$ c, with a strict structure. This allows for more organized code. More on sequential models can be found \href{https://www.tensorflow.org/guide/keras/sequential_model}{here}.}, but at a cost of flexibility and lack of potential non-linear structure in the architecture. \par 
Certain features where removed, as they did not have a good enough overlap between MC and ATLAS data. All features where checked to ensure as good correspondence as possible, and the figures for all the features can be found in the \href{https://github.com/Gadangadang/FYS5555/tree/main/Project3}{Github repo} for this project, under the "figures/implementation" folder. The features that were chosen to exclude where lep1\_tracksigd0pvunbiased and lep2\_tracksigd0pvunbiased, which are shown below:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/implementation/lep1_tracksigd0pvunbiased.pdf}
    \caption{Lep1 tracksigd0unbiased histogram for background mc and ATLAS data. }
    \label{fig:lep1sigd0}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/implementation/lep2_tracksigd0pvunbiased.pdf}
    \caption{Lep2 tracksigd0unbiased histogram for background mc and ATLAS data.}
    \label{fig:lep2sigd0}
\end{figure}

We see in figures \ref{fig:lep1sigd0} and \ref{fig:lep2sigd0} that both have excess in the data compared to the background mc. 


\section{Results}

\subsection{The first model}

The first auto encoder model was found with a hyper parameter search and is structured as shown in figure \ref{fig:big_ae_plot} below. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/results/models/ae_model_plot_big_rm3feats.pdf}
    \caption{Auto encoder architecture after hyper parameter search. }
    \label{fig:big_ae_plot}
\end{figure}
The hyper parameter search gave the optimal hyper parameters as follows: the encoder has the following activation functions: tanh, linear, linear, leakyReLU, and the decoder has the following activation functions: relu, leakyReLU, tanh, and linear. The optimal learning rate was 0.0015, the optimal kernel regularizer was 0.05, the optimal activity regularizer was 0.5 and the optimal alpha for the leakyReLU was 1. These hyper parameters was found after around 30 minutes of searching, and then stopped, as it appeared that the validation MSE did not improve much. Thus, there are most likely a better set of hyper parameters that could get an even lower validation MSE. \par 
The reconstruction error of ATLAS data is shown in figure \ref{fig:data_b_big_pred}.

\begin{figure}[H]
     \centering
         \includegraphics[width=0.46\textwidth]{figures/results/predictions/b_data_recon_big_rm3feats_loglog.pdf}
         \caption{Log log histogram showing reconstruction error for ATLAS data and background mc for big auto encoder. }
     \label{fig:data_b_big_pred}
\end{figure}

In figure \ref{fig:data_b_big_pred} we see that the auto encoder gets a good overlap of reconstruction error between ATLAS data and background mc. This is in accordance with the fact that the ATLAS collaboration have with very high certainty only found SM data in the ATLAS Open Data. We also observe here that the algorithm groups events with different error, as shown with the camel back pattern. This pattern repeats several times in figure \ref{fig:data_b_big_pred}, which indicates that the algorithm learned its own categories for the standard model. Using all the signal mc samples, I can test if the classifier can distinguish new physics signal mc from sm processes by mere reconstruction. The reconstruction error is shown in figure \ref{fig:s_b_big_pred_allsig}.


\begin{figure}[H]
     \centering
         \includegraphics[width=0.46\textwidth]{figures/results/predictions/b_s_recon_big_rm3feats_loglog.pdf}
    \caption{Log log histogram showing reconstruction error of background mc and signal mc for big auto encoder with all signal samples.  }
    \label{fig:s_b_big_pred_allsig}
\end{figure}


\begin{figure}[H]    
    
    \centering
         \includegraphics[width=0.46\textwidth]{figures/results/rocs/b_s_roc_curve_big_allsig.pdf}
         \caption{ROC curve signal mc and background mc for big auto encoder with all signal samples.}
         \label{fig:s_b_big_roc_allsig}
\end{figure}


From these results in figure \ref{fig:s_b_big_pred_allsig} and \ref{fig:s_b_big_roc_allsig} it appears that the auto encoder very well manages to separate signal from background, having only been shown background mc. However, there is some problems with these results. The Randall-Sundrum Graviton to dimuons samples in the signal mc were given with very high weights compared to the Graviton to dielectrons samples, which highly indicates that the data samples are corrupt somehow. Thus we have to check the reconstruction error on signal mc without the Randall-Sundrum Graviton samples. This is shown in figure \ref{fig:roc_sig_big_allsig_nogmumu}.


\begin{figure}[H]    
     \centering
    \includegraphics[width=0.46\textwidth]{figures/results/predictions/b_s_recon_big_rm3_feats.pdf}
    \caption{Log log histogram showing reconstruction error of background mc and signal mc for big auto encoder without Graviton to dimuons.  }
    \label{fig:s_b_big_allsig_nogmumu}
\end{figure}

\begin{figure}[H]    
  \centering
         \includegraphics[width=0.46\textwidth]{figures/results/rocs/b_s_roc_curve_big_rm3feats.pdf}
         \caption{ROC curve signal mc and background mc for big auto encoder without Graviton to dimuons. }
         \label{fig:roc_sig_big_allsig_nogmumu}
\end{figure}


In figure \ref{fig:s_b_big_allsig_nogmumu} and \ref{fig:roc_sig_big_allsig_nogmumu} clearly see that the auto encoder struggles to separate background and signal mc, with a AUC score of around 0.58. This shows that for signals that are fairly similar to background mc, this model will struggle to tell them apart. Here we also observe the camel back structure, where the algorithm has grouped certain events together. There are possibilities to try to mediate this. 

\subsection{Attempts at improvement}

\subsubsection{Unstacked auto encoder}
One such method is altering the architecture of the auto encoder. To compare this architecture I created a smaller auto encoder, shown in figure \ref{fig:small_ae_plot}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/results/models/ae_model_plot_small.pdf}
    \caption{Smaller auto encoder architecture after hyper parameter search. }
    \label{fig:small_ae_plot}
\end{figure}

The hyper parameters used for this model was a learning rate of 0.9, 24 nodes in the latent space, the activation function for the latent space was tanh, the activation function for the output was linear, the kernel regularization was 0.05 and the activation regularization was 0.01. The model was trained for 6 epochs. 


\begin{figure}[H]
     \centering
         \includegraphics[width=0.46\textwidth]{figures/results/predictions/b_data_recon_small_rm3_feats_nogmumu.pdf}
         \caption{Log log histogram showing reconstruction error for ATLAS data and background mc for small auto encoder. }
     \label{fig:data_b_small_pred}
\end{figure}

Using the architecture in figure \ref{fig:small_ae_plot} I get a good overlap of ATLAS data and background mc, with a center of about 4 MSE reconstruction error. Here we see that the algorithm creates what appears to be one group. 

\begin{figure}[H]    
 \centering
    \includegraphics[width=0.46\textwidth]{figures/results/predictions/b_s_recon_small_rm3_feats_nogmumu_nolog.pdf}
    \caption{Log log histogram showing reconstruction error of background mc and signal mc for small auto encoder. }
    \label{fig:s_b_small_pred_}   
  
\end{figure}

\begin{figure}[H]    
  \centering
         \includegraphics[width=0.46\textwidth]{figures/results/rocs/b_s_roc_curve_small_nogmumu.pdf}
         \caption{ROC curve signal mc and background mc for small auto encoder}
         \label{fig:s_b_small_roc}  
  
\end{figure}


As shown in figure \ref{fig:s_b_small_pred_} and \ref{fig:s_b_small_roc} we observe that even though this model gets a good overlap, it struggles even further than the big auto encoder, having an AUC score of only 0.48 which is only slightly better than guessing. Here we observe that the algorithm created one group with all events for signal and background just as it did with ATLAS data. This could mean that such a simple model needs more complexity in terms of layers and node reduction per layer to be able to learn how to separate. \par 

\subsubsection{Reduced features in data set with stacked auto encoder}

Another idea is removing low level features that are fairly similar for background and signal mc. From testing it was found that even with as low as 13 features, the output from the algorithm only improved slightly.



\begin{figure}[H]    
  \centering
         \includegraphics[width=0.46\textwidth]{figures/results/models/ae_model_plot_13feats_big.pdf}
         \caption{Auto encoder architecture found by hyper parameter search with 13 features in the input.}
         \label{fig:ae_13feats_big}  
  
\end{figure}

In figure \ref{fig:ae_13feats_big} we have the tuned auto encoder architecture based on a hyper parameter search for 13 feature dataset. The hyper parameters were as follows. The encoder had leakyrelu, linear, leakyrelu as activation functions. The latent space had 2 nodes and linear as activation function. The decoder had leakyrelu, tanh, relu and linear as activation functions. The kernel regularization was 0.5, the activation regularization was 0.5, the leakyrelu alpha parameter was 0.1 and the learning rate was 0.0015. The model was trained with 5 epochs.

\begin{figure}[H]
     \centering
         \includegraphics[width=0.46\textwidth]{figures/results/predictions/b_data_recon_big_rm3_feats_rmmanyfeats_loglog.pdf}
         \caption{Log log histogram showing reconstruction error for ATLAS data and background mc for big auto encoder using only 13 features.}
     \label{fig:data_b_big_pred_13feats}
\end{figure}

In figure \ref{fig:data_b_big_pred_13feats} we observe that the reconstruction error of ATLAS data and background mc have fairly good overlap. As expected the reconstruction distribution is centered at a much lower value than for the larger data sets, as there are a lot fewer features to reconstruct. We also see here that the algorithm separates some of the events into at least two groups, with a clear split around 1 MSE. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/results/predictions/b_s_recon_big_rm3_feats_rmmanyfeats_loglog.pdf}
    \caption{Log log histogram showing reconstruction error of background mc and signal mc for big auto encoder using only 13 features. }
    \label{fig:roc_sig_big_13feats}
\end{figure}

\begin{figure}[H]
    \centering
         \includegraphics[width=0.46\textwidth]{figures/results/rocs/b_s_roc_curve_big_rmmanyfeats.pdf}
         \caption{ROC curve signal mc and background mc for big auto encoder using only 13 features.}
         \label{fig:s_b_big_roc_13}
\end{figure}



From figure \ref{fig:roc_sig_big_13feats} we observe that even with only 13 features, 
[njet60,  nbjet85, isOS,isSF,  mt2, met\_et,  , lep1\_pt,lep1\_E, lep1\_ptcone30, lep1\_etcone20, lep2\_pt,  lep2\_E, costhstar], the auto encoder struggles to separate background and signal mc. There are about 10 events around -0.3 MSE reconstruction error that it cannot match with background mc, but the rest of the data overlaps fairly well. This also matches the reconstruction of ATLAS data, where this area has no events. The ROC score for the auto encoder was around 0.59, which only slightly better than for the dataset with all 33 features. \par

\subsubsection{Auto encoder with standard scaling}

A third option is to change the scaling and look at its effect on the handling of data. The previous models have all used MinMax scaling to scale the data, where as this model used standard scaling instead. Using standard scaling I found the following results.

\begin{figure}[H]
     \centering
         \includegraphics[width=0.46\textwidth]{figures/results/models/ae_model_plot_big_rm3feats_standard_propsigbacksplit.pdf}
         \caption{Auto encoder architecture after tuning of hyper parameters using standard scaling on the datasets. }
     \label{fig:ae_big_standard}
\end{figure}

In figure \ref{fig:ae_big_standard} we have the architecture for the auto encoder after hyper parameter tuning where the datasets are scaled using standard scaling. The hyperparameters were as follows. The activation functions for the encoder were tanh on all 4 layers. The activation functions for the decoder were leakyrelu for the first two layers and tanh for the last two layers. The kernel regularization, activation regularization and leaky relu alpha parameter were all 0.01, and the learning rate as 0.0015. The model was trained with 15 epochs.

\begin{figure}[H]
     \centering
         \includegraphics[width=0.46\textwidth]{figures/results/predictions/b_data_recon_big_rm3_feats_standard_propsigbacksplit.pdf}
         \caption{Log log histogram showing reconstruction error for ATLAS data and background mc for big auto encoder, using standard scaling.}
     \label{fig:data_b_big_pred_stand}
\end{figure}

In figure \ref{fig:data_b_big_pred_stand} we see that even though the reconstruction error for ATLAS data and background mc has a wide spread, it overlaps well, which indicates that the autoencoder learned to reproduce the standard model, even though some events where much harder to reproduce than others. We also note here that only a few 10's to 100's of events are above $10^{3.5}$. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/results/predictions/b_s_recon_big_rm3_feats_standard_propsigbacksplit_nolog_allsig_nogmumu.pdf}
    \caption{Log log histogram showing reconstruction error of background mc and signal mc for big auto encoder, using standard scaling. }
    \label{fig:s_b_big_pred_stand}
\end{figure}

\begin{figure}[H]
    \centering
         \includegraphics[width=0.46\textwidth]{figures/results/rocs/b_s_roc_curve_big_standard_allsig_nogmumu.pdf}
         \caption{ROC curve signal mc and background mc for big auto encoder, using standard scaling.}
         \label{fig:s_b_big_roc_stand}
\end{figure}



In figure \ref{fig:s_b_big_pred_stand} we observe that the model somewhat overlaps the background and signal mc, which the exception of a few thousand samples in the second peak. It appears here that the model manages to separate some signal events from the background mc. This model preformed better than the models using Min Max scaling, with a AUC score of about 0.69. \par 

\section{Discussion and possible ways forward}
There were several issues with training and tuning the model, much of which had to do with hardware. The background and signal mc data set alone was around 17 Gb of data in hdf5 format. To do training, prediction and hyper parameter searches especially, GPU usage was needed. With Tensorflow's XLA\footnote{XLA architecture can be found \href{https://www.tensorflow.org/xla/architecture}{here}, and XLA api can be found \href{https://www.tensorflow.org/xla}{here}.} (Accelerated Linear Algebra), the algorithm got a speedup from 1 hour and 30 minutes to around 1 minute per epoch when training on 80\% of the background mc. Still, because of the enormous amount of data, the hyper parameter search had to be done using a smaller sample size. Thus each epoch only took 10-30 seconds, which allowed for many iterations of hyperparameter tuning. There is however the issue that the random sampling does not guarantee accurately representing the sm background sample distribution, and thus the hyperparameters could very well have been even better tuned.  \par \par

It is also worth noting that the ROC curves and reconstruction error figure using the signal mc only in part shows how the auto encoder performs. The samples does not represent all possible new physics, and thus we can only conclude from our results that the auto encoder struggles to separate those signal mc samples. The auto encoder has one metric, and that is minimizing reconstruction error. If you then have a signal that looks very similar to background, then the reconstruction error will be similar to the background. This does not mean that the auto encoder has not learned some hidden relations in the data, only that those relations are not enough for those kinds of signals. In the event that other signals appears in detector data, the auto encoder could still find them. \par \par

Another crucial point to make is that in the realm of machine learning, there are few guidelines for how the architecture should look. In this case with the auto encoder, the only requirement is that in the encoder the previous layers must have at least one more node than the one after, and in the decoder the layer after must have at least one more node than the one before it. With respect to the amount of layers, regularization of layers towards bias or output, implementation of dropout layers, initialization of weights, or symmetry with regards to encoder and decoder architecture, an enormous hyper parameter search is the only way to find the ideal model. This bias that the designer of the given auto encoder brings is very hard to quantify, but is indeed crucial, and generates a gigantic pool of possibilities which, given enough time and resources could yield valuable insight. \par \par

Scaling also had an interesting impact on the output, as it appears that standard scaling allows for better detection of anomalies. This choice of scaling improved the AUC score from around 0.58 to around 0.69, which is a significant improvement. One reason for why this scaling is better might be that outliers stick out more with this scaling method, due to the centering around 0. It is also important to note here that these results are all done using all signal samples, except for Graviton to dimuons. Ideally one should have had individual predictions on all the signal mc that we have to see if there are differences in the prediction, thus allowing us to see what kind of signals the auto encoder struggles to filter out and vice versa. \par \par

The choice of features also contributes to the effect of the auto encoder. The removal of the $d_0$ significance for both leptons was done by looking at the distribution of data and background mc by checking the difference between them, and was done by eye sight. This could certainly have been done better, for instance by requiring a threshold for how large the ratio between data and background mc can be. \par \par



From the results above there is a case for that auto encoder should not be ruled out as a possibly good candidate for new physics search, as there are still many aspects that are not well understood. A good start would be to further study how the model learns to merge and reconstruct each feature, and if it quickly converges towards only focusing on one or a few main features. If this is better understood, then maybe the construction of certain features in the preparation of the background mc and ATLAS data could be made to handle this better. It could very well be that certain features or combinations of certain features are two similar for the algorithm to tell the events apart. \par 
Another point of interest is the relation between number of features and complexity of the architecture. From the models tested above, it would appear that a stacked and more complex auto encoder is needed for the algorithm to learn to reproduce the dataset with out copying it. It is also not clear if a symmetric architecture is more ideal than a non symmetric architecture, and a hyper parameter search with this included could yield interesting results. Another important, but computationally heavy, task is to do an extensive hyper parameter search, using a lot more background mc, to check ideal structure in terms (non)-symmetry or amount of layers in architecture, number of nodes, activation functions etc, as mentioned earlier. \par 
The camel back and box pattern shown in the results are also of interest. In the figures background mc was plotted as one dataset, but it would be very interesting to see which of the samples that were easier and harder to reproduce. This could also give more insight into which classes of signal models that would be easier to find using auto encoders. \par Finally, to search for new physics using an auto encoder, there should be better insight as to what "classes" of new physics final states that the algorithm could better filter out. Suppose you compare two candidates for new physics and they are somewhat different, it might be that certain auto encoder could have the architecture altered to better filter out certain signal classes, and not a specific model, thus preserving the semi unsupervised part.   \par \par


\section{Conclusion}
This report attempted to create an auto encoder that manages to high accuracy reconstruct standard model processes based on training done on background mc and with poor reconstruction on collective anomalies given as signal mc samples. All machine learning was done using the Tensorflow api, with functional structure. Several models where tuned using Keras-Tuner to get the semi-optimal hyperparameters, and whilst all of the models successfully managed to have good overlap between background mc and ATLAS Open data, only the autoencoder trained on standard scaled datasets managed to separate out signals. We found that the ROC score increased both when using a multi layer auto encoder and by using this standard scaling. Based on the findings in this report, the auto encoder cannot be completely disregarded as a useful method for anomaly detection in high energy physics, as there are still a lot about the algorithm that is not well understood. Further research into an extensive hyperparameter search, understanding could yield very interesting results. 



\mbox{~}
\onecolumngrid
\printbibliography

\newpage
\appendix
\section{Features}\label{appendix:features}
% Define new columns types 
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}} % left fixed width
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}} % center fixed width
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}} % flush right fixed width
\begin{table}[H]
    % \setlength{\tabcolsep}{15pt}
    \renewcommand{\arraystretch}{1.3}
    \begin{center}
    \caption{Features and their description \cite{ATL-OREACH-PUB-2020-001}}
    \begin{tabular}{|C{4.5cm}|C{4.5cm}|L{9.5cm}|} \hline
    
    Feature & Type & Description \\ \hline
    njet20 & int & Number of jets with $p_T > 20$ GeV\\ \hline
    njet60 & int & Number of jets with $p_T > 60$ GeV  \\ \hline
    nbjet60 & int & Number of b-jets with $p_T > 60$ GeV  \\ \hline
    nbjet70 & int & Number of b-jets with   \\ \hline
    nbjet77 & int & Number of b-jets with   \\ \hline
    nbjet85 & int & Number of b-jets with   \\ \hline
    isOS  & int & 1 if leptons have opposite charge, 0 if leptons have same charge \\ \hline
    isSF & int & 1 if leptons are of same flavor, 0 is leptons are of different flavor, flavor code 11 is electron, flavor code 13 is muon \\ \hline
    mll & float & Invariant mass of the two leptons \\ \hline
    mt2 & float & The maximal lower bound on the mass of each member of a
    pair of identical parent particles which, if pairproduced at a hadron collider, could have each
    undergone a two-body decay into (i) a visible
    particle (or collection of particles) and (ii) an
    invisible object of hypothesised mass $\chi$\cite{Lester_2011}.\\ \hline
    met\_et & float & Transverse energy of the missing momentum vector \\ \hline
    met\_phi & float & Azimuthal angle of the missing momentum vector\\ \hline
    lep\_flav & vector<int> & Flavor of the lepton, 11 for electron and 13 for muon \\ \hline
    lep\_pt & vector<float> & Vector containing transverse momentum for the leptons \\ \hline
    lep\_eta & vector<float> & Vector containing pseudo-rapidity , $\eta$, for the leptons \\ \hline
    lep\_phi & vector<float> & Vector containing azimuthal angle, $\phi$, for the leptons \\ \hline
    lep\_E & vector<float> & Vector containing the energy for the leptons \\ \hline
    lep\_ptcone30 & vector<float>  & Vector containing scalar sum of track $p_T$ in a cone of $R=0.3$ around lepton, used for tracking isolation \\ \hline
    lep\_etcone20 & vector<float>  & Vector containing scalar sum of track $E_T$ in a cone of $R=0.2$ around lepton, used for calorimeter isolation \\ \hline
    lep\_trackd0pvunbiased  & vector<float> & $d_0$ of track associated to lepton at point of closest approach (p.c.a.)\\ \hline
    lep\_tracksigd0pvunbiased & vector<float> & $d_0$ significance of the track associated to lepton at the p.c.a.\\ \hline
    lep\_isTightID & vector<bool> & Vector containing boolean indicating whether leptons satisfies tight ID reconstruction criteria\\ \hline
    lep\_z0 & vector<float> & Vector containing z-coordinate of the track associated for the leptons wrt. primary vertex \\ \hline
    
    channelNumber & int & Data sample ID\\\hline
    costhstar & float & Cosine of dilepton decay angle\\ \hline
    weight & float & MC sample weight \\ \hline
    category & string & SM or BSM category \\ \hline
    physdescr & string & MC process name \\ \hline
    isSignal & int & 1 if category is a BSM signal, 0 if the category is SM background \\ \hline
            
    \end{tabular}
    \label{tab:feature_table}
  \end{center}
\end{table}



\end{document}